{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Split Federated Learning with Differential Privacy Experiments\n",
        "\n",
        "This notebook compares Vanilla DP and Adaptive DP across different experimental settings:\n",
        "- Different number of clients\n",
        "- IID vs non-IID data distribution\n",
        "- Various models and datasets\n",
        "\n",
        "Results are logged to a CSV file for further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup environment and install dependencies\n",
        "%pip install torch torchvision matplotlib pandas tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/yourusername/Comp430_Project.git\n",
        "%cd Comp430_Project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import copy\n",
        "from collections import OrderedDict\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "We'll use several popular datasets for our experiments:\n",
        "1. MNIST - Handwritten digits\n",
        "2. CIFAR-10 - Small color images in 10 classes\n",
        "3. Fashion-MNIST - Fashion items similar to MNIST format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to load different datasets\n",
        "def load_dataset(dataset_name, data_dir='./data'):\n",
        "    \"\"\"\n",
        "    Load and preprocess datasets.\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: Name of the dataset ('mnist', 'cifar10', 'fashion_mnist')\n",
        "        data_dir: Directory to store datasets\n",
        "        \n",
        "    Returns:\n",
        "        train_dataset, test_dataset\n",
        "    \"\"\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    \n",
        "    if dataset_name.lower() == 'mnist':\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "        \n",
        "        train_dataset = datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
        "        \n",
        "    elif dataset_name.lower() == 'cifar10':\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "        \n",
        "        train_dataset = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
        "        \n",
        "    elif dataset_name.lower() == 'fashion_mnist':\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.2860,), (0.3530,))\n",
        "        ])\n",
        "        \n",
        "        train_dataset = datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.FashionMNIST(root=data_dir, train=False, download=True, transform=transform)\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "    \n",
        "    return train_dataset, test_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data partitioning for IID and non-IID settings\n",
        "\n",
        "def partition_data_iid(dataset, num_clients):\n",
        "    \"\"\"\n",
        "    Partitions a dataset into IID subsets for each client.\n",
        "    \"\"\"\n",
        "    num_items_per_client = len(dataset) // num_clients\n",
        "    client_datasets = {}\n",
        "    all_indices = list(range(len(dataset)))\n",
        "    np.random.shuffle(all_indices)  # Shuffle indices for random distribution\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        start_idx = i * num_items_per_client\n",
        "        # Ensure the last client gets any remaining data points\n",
        "        end_idx = (i + 1) * num_items_per_client if i != num_clients - 1 else len(dataset)\n",
        "        client_indices = all_indices[start_idx:end_idx]\n",
        "        client_datasets[i] = Subset(dataset, client_indices)\n",
        "\n",
        "    return client_datasets\n",
        "\n",
        "def partition_data_dirichlet(dataset, num_clients, alpha):\n",
        "    \"\"\"\n",
        "    Partitions the data using a Dirichlet distribution to create non-IID data splits.\n",
        "    \n",
        "    Args:\n",
        "        dataset: The dataset to partition (must have a 'targets' attribute)\n",
        "        num_clients: Number of clients to create partitions for\n",
        "        alpha: Dirichlet concentration parameter - controls skew\n",
        "               alpha→0: extreme skew, each client gets mostly one class\n",
        "               alpha→∞: balanced distribution (IID)\n",
        "    \n",
        "    Returns:\n",
        "        A dictionary mapping client IDs to dataset subsets\n",
        "    \"\"\"\n",
        "    if not hasattr(dataset, 'targets'):\n",
        "        if hasattr(dataset, 'labels'):\n",
        "            targets = np.array(dataset.labels)\n",
        "        else:\n",
        "            # For torchvision datasets\n",
        "            targets = np.array(dataset.targets)\n",
        "    else:\n",
        "        targets = np.array(dataset.targets)\n",
        "        \n",
        "    classes = np.unique(targets)\n",
        "    idx_by_class = {c: np.where(targets == c)[0] for c in classes}\n",
        "\n",
        "    client_indices = [[] for _ in range(num_clients)]\n",
        "\n",
        "    for c in classes:\n",
        "        # draw class proportions\n",
        "        props = np.random.dirichlet(alpha * np.ones(num_clients))\n",
        "        props = (props * len(idx_by_class[c])).astype(int)\n",
        "        \n",
        "        # If there's rounding error, adjust last partition size\n",
        "        props[-1] = len(idx_by_class[c]) - props[:-1].sum()\n",
        "\n",
        "        # split indices\n",
        "        np.random.shuffle(idx_by_class[c])\n",
        "        start = 0\n",
        "        for cid, cnt in enumerate(props):\n",
        "            client_indices[cid].extend(idx_by_class[c][start:start+cnt])\n",
        "            start += cnt\n",
        "\n",
        "    return {cid: Subset(dataset, idx) for cid, idx in enumerate(client_indices)}\n",
        "\n",
        "def get_client_dataloaders(dataset, num_clients, batch_size, distribution='iid', alpha=1.0):\n",
        "    \"\"\"\n",
        "    Creates DataLoaders for each client based on specified distribution.\n",
        "    \n",
        "    Args:\n",
        "        dataset: Dataset to partition\n",
        "        num_clients: Number of clients\n",
        "        batch_size: Batch size for DataLoaders\n",
        "        distribution: 'iid' or 'dirichlet'\n",
        "        alpha: Concentration parameter for Dirichlet distribution (used only if distribution='dirichlet')\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary mapping client IDs to their respective DataLoaders\n",
        "    \"\"\"\n",
        "    if distribution == 'iid':\n",
        "        client_datasets = partition_data_iid(dataset, num_clients)\n",
        "    elif distribution == 'dirichlet':\n",
        "        client_datasets = partition_data_dirichlet(dataset, num_clients, alpha)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown distribution method: {distribution}\")\n",
        "        \n",
        "    client_loaders = {}\n",
        "    for client_id, dataset in client_datasets.items():\n",
        "        client_loaders[client_id] = DataLoader(\n",
        "            dataset, batch_size=batch_size, shuffle=True, \n",
        "            pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "        \n",
        "    return client_loaders\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Model Definitions\n",
        "\n",
        "We'll use several model architectures suitable for image classification:\n",
        "1. Simple CNN for MNIST\n",
        "2. Deeper CNN for CIFAR-10\n",
        "3. MLP as a baseline model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define various models for our experiments\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"Simple CNN suitable for MNIST and Fashion-MNIST.\"\"\"\n",
        "    def __init__(self, num_classes=10, split_layer=1):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.split_layer = split_layer\n",
        "        \n",
        "        # Client-side layers (WC)\n",
        "        self.client_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2)\n",
        "            )\n",
        "        ])\n",
        "        \n",
        "        # Server-side layers (WS)\n",
        "        self.server_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(32 * 7 * 7, 128),\n",
        "                nn.ReLU()\n",
        "            ),\n",
        "            nn.Linear(128, num_classes)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Apply client-side layers up to split_layer\n",
        "        for i in range(self.split_layer):\n",
        "            if i < len(self.client_layers):\n",
        "                x = self.client_layers[i](x)\n",
        "        \n",
        "        # Apply server-side layers\n",
        "        for layer in self.server_layers:\n",
        "            x = layer(x)\n",
        "            \n",
        "        return x\n",
        "    \n",
        "    def client_forward(self, x):\n",
        "        \"\"\"Forward pass through client part of the model.\"\"\"\n",
        "        for i in range(self.split_layer):\n",
        "            if i < len(self.client_layers):\n",
        "                x = self.client_layers[i](x)\n",
        "        return x\n",
        "    \n",
        "    def server_forward(self, x):\n",
        "        \"\"\"Forward pass through server part of the model.\"\"\"\n",
        "        for layer in self.server_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class CIFARCNN(nn.Module):\n",
        "    \"\"\"Deeper CNN suitable for CIFAR-10.\"\"\"\n",
        "    def __init__(self, num_classes=10, split_layer=2):\n",
        "        super(CIFARCNN, self).__init__()\n",
        "        self.split_layer = split_layer\n",
        "        \n",
        "        # Client-side layers (WC)\n",
        "        self.client_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(32)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.BatchNorm2d(64)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.BatchNorm2d(128)\n",
        "            )\n",
        "        ])\n",
        "        \n",
        "        # Server-side layers (WS)\n",
        "        self.server_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(128 * 8 * 8, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.5)\n",
        "            ),\n",
        "            nn.Linear(256, num_classes)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Apply client-side layers up to split_layer\n",
        "        for i in range(self.split_layer):\n",
        "            if i < len(self.client_layers):\n",
        "                x = self.client_layers[i](x)\n",
        "        \n",
        "        # Apply server-side layers\n",
        "        for layer in self.server_layers:\n",
        "            x = layer(x)\n",
        "            \n",
        "        return x\n",
        "    \n",
        "    def client_forward(self, x):\n",
        "        \"\"\"Forward pass through client part of the model.\"\"\"\n",
        "        for i in range(self.split_layer):\n",
        "            if i < len(self.client_layers):\n",
        "                x = self.client_layers[i](x)\n",
        "        return x\n",
        "    \n",
        "    def server_forward(self, x):\n",
        "        \"\"\"Forward pass through server part of the model.\"\"\"\n",
        "        for layer in self.server_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Baseline MLP model.\"\"\"\n",
        "    def __init__(self, input_dim=784, hidden_dim=256, num_classes=10, split_layer=1):\n",
        "        super(MLP, self).__init__()\n",
        "        self.split_layer = split_layer\n",
        "        \n",
        "        # Client-side layers (WC)\n",
        "        self.client_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(input_dim, hidden_dim),\n",
        "                nn.ReLU()\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "        ])\n",
        "        \n",
        "        # Server-side layers (WS)\n",
        "        self.server_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.ReLU()\n",
        "            ),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Apply client-side layers up to split_layer\n",
        "        for i in range(self.split_layer):\n",
        "            if i < len(self.client_layers):\n",
        "                x = self.client_layers[i](x)\n",
        "        \n",
        "        # Apply server-side layers\n",
        "        for layer in self.server_layers:\n",
        "            x = layer(x)\n",
        "            \n",
        "        return x\n",
        "    \n",
        "    def client_forward(self, x):\n",
        "        \"\"\"Forward pass through client part of the model.\"\"\"\n",
        "        for i in range(self.split_layer):\n",
        "            if i < len(self.client_layers):\n",
        "                x = self.client_layers[i](x)\n",
        "        return x\n",
        "    \n",
        "    def server_forward(self, x):\n",
        "        \"\"\"Forward pass through server part of the model.\"\"\"\n",
        "        for layer in self.server_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "def get_model(model_name, dataset_name, split_layer=1):\n",
        "    \"\"\"\n",
        "    Returns a model instance based on model_name and dataset_name.\n",
        "    \n",
        "    Args:\n",
        "        model_name: 'simple_cnn', 'cifar_cnn', 'mlp'\n",
        "        dataset_name: 'mnist', 'cifar10', 'fashion_mnist'\n",
        "        split_layer: Layer index at which to split the model\n",
        "        \n",
        "    Returns:\n",
        "        Model instance\n",
        "    \"\"\"\n",
        "    if model_name == 'simple_cnn':\n",
        "        if dataset_name in ['mnist', 'fashion_mnist']:\n",
        "            return SimpleCNN(num_classes=10, split_layer=split_layer)\n",
        "        else:\n",
        "            raise ValueError(f\"SimpleCNN not suitable for {dataset_name}\")\n",
        "            \n",
        "    elif model_name == 'cifar_cnn':\n",
        "        if dataset_name == 'cifar10':\n",
        "            return CIFARCNN(num_classes=10, split_layer=split_layer)\n",
        "        else:\n",
        "            raise ValueError(f\"CIFARCNN not suitable for {dataset_name}\")\n",
        "            \n",
        "    elif model_name == 'mlp':\n",
        "        if dataset_name in ['mnist', 'fashion_mnist']:\n",
        "            return MLP(input_dim=784, hidden_dim=256, num_classes=10, split_layer=split_layer)\n",
        "        elif dataset_name == 'cifar10':\n",
        "            return MLP(input_dim=3*32*32, hidden_dim=512, num_classes=10, split_layer=split_layer)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown dataset for MLP: {dataset_name}\")\n",
        "            \n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Privacy Mechanisms and Accountants\n",
        "\n",
        "Now we'll implement:\n",
        "1. Laplace mechanism for activations\n",
        "2. Gaussian mechanism for gradients\n",
        "3. Privacy accountant for tracking ε and δ\n",
        "4. Vanilla vs Adaptive DP approaches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Noise utilities\n",
        "\n",
        "def laplace_eps(scale: float, sensitivity: float) -> float:\n",
        "    \"\"\"Compute epsilon for Laplace mechanism.\"\"\"\n",
        "    return sensitivity / scale\n",
        "\n",
        "def add_laplacian_noise(tensor, sensitivity, epsilon_prime, device='cuda'):\n",
        "    \"\"\"\n",
        "    Adds Laplacian noise to a tensor based on sensitivity and epsilon_prime.\n",
        "    \n",
        "    Args:\n",
        "        tensor: The input tensor (e.g., activations).\n",
        "        sensitivity: The L1 sensitivity of the function outputting the tensor.\n",
        "        epsilon_prime: The privacy budget epsilon' for this noise addition step.\n",
        "        device: The device to generate noise on.\n",
        "\n",
        "    Returns:\n",
        "        Tensor with added Laplacian noise.\n",
        "    \"\"\"\n",
        "    # If sensitivity is zero, no noise is added.\n",
        "    if sensitivity == 0.0:\n",
        "        return tensor\n",
        "\n",
        "    if epsilon_prime <= 0:\n",
        "        raise ValueError(\"Epsilon prime must be positive for Laplacian noise.\")\n",
        "\n",
        "    scale = sensitivity / epsilon_prime\n",
        "    assert abs(laplace_eps(scale, sensitivity) - epsilon_prime) < 1e-5, \\\n",
        "        \"Laplace ε′ mismatch → wrong sensitivity or ε′\"\n",
        "\n",
        "    # Generate Laplacian noise using PyTorch distributions\n",
        "    laplace_dist = torch.distributions.laplace.Laplace(loc=0.0, scale=scale)\n",
        "    noise = laplace_dist.sample(tensor.size()).to(device)\n",
        "\n",
        "    return tensor + noise\n",
        "\n",
        "def add_gaussian_noise(tensor, clip_norm, noise_multiplier, device='cuda'):\n",
        "    \"\"\"\n",
        "    Adds Gaussian noise scaled by clip_norm and noise_multiplier.\n",
        "    Used after summing clipped per-sample gradients.\n",
        "    \n",
        "    Args:\n",
        "        tensor: The input tensor (e.g., summed clipped gradients).\n",
        "        clip_norm: The L2 norm bound C used for clipping.\n",
        "        noise_multiplier: The noise multiplier z (sigma = z * C).\n",
        "        device: The device to generate noise on.\n",
        "\n",
        "    Returns:\n",
        "        Tensor with added Gaussian noise.\n",
        "    \"\"\"\n",
        "    if noise_multiplier < 0:\n",
        "        raise ValueError(\"Noise multiplier cannot be negative.\")\n",
        "    if clip_norm <= 0:\n",
        "        # Allow clip_norm=0 for cases where no privacy is applied (noise_multiplier=0)\n",
        "        if noise_multiplier > 0:\n",
        "             raise ValueError(\"Clip norm must be positive if noise multiplier is positive.\")\n",
        "        else:\n",
        "            # No clipping, no noise - return original tensor\n",
        "            return tensor\n",
        "\n",
        "    sigma = noise_multiplier * clip_norm\n",
        "\n",
        "    # If sigma is zero, no noise is added.\n",
        "    if sigma == 0.0:\n",
        "        return tensor\n",
        "\n",
        "    # Generate Gaussian noise N(0, sigma^2 * I)\n",
        "    gaussian_dist = torch.distributions.normal.Normal(loc=0.0, scale=sigma)\n",
        "    noise = gaussian_dist.sample(tensor.size()).to(device)\n",
        "\n",
        "    return tensor + noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Privacy Accountant\n",
        "\n",
        "def _log_comb(n: int, k: int) -> float:\n",
        "    \"\"\"Computes log(C(n, k)) using log gamma functions.\"\"\"\n",
        "    if k < 0 or k > n:\n",
        "        return -float('inf') # Log of zero\n",
        "    # Using math.lgamma which computes log(Gamma(x)) = log((x-1)!)\n",
        "    # log(n! / (k! * (n-k)!)) = lgamma(n+1) - lgamma(k+1) - lgamma(n-k+1)\n",
        "    return math.lgamma(n + 1) - math.lgamma(k + 1) - math.lgamma(n - k + 1)\n",
        "\n",
        "def _log_add_exp(log_a: float, log_b: float) -> float:\n",
        "    \"\"\"Computes log(exp(log_a) + exp(log_b)) robustly.\"\"\"\n",
        "    if log_a == -float('inf'):\n",
        "        return log_b\n",
        "    if log_b == -float('inf'):\n",
        "        return log_a\n",
        "    if log_a > log_b:\n",
        "        return log_a + math.log1p(math.exp(log_b - log_a))\n",
        "    else:\n",
        "        return log_b + math.log1p(math.exp(log_a - log_b))\n",
        "\n",
        "def _compute_rdp_epsilon_step(q: float, noise_multiplier: float, alpha: int) -> float:\n",
        "    \"\"\"\n",
        "    Computes the Renyi Differential Privacy (RDP) epsilon for a single step\n",
        "    of the sampled Gaussian mechanism.\n",
        "    \"\"\"\n",
        "    if q == 0:\n",
        "        return 0.0 # No privacy cost if not sampled\n",
        "    if q == 1.0:\n",
        "        # Standard (non-sampled) Gaussian mechanism RDP\n",
        "        if noise_multiplier == 0:\n",
        "             return float('inf') # Infinite privacy cost with zero noise\n",
        "        # RDP is alpha / (2 * sigma^2) where sigma is noise_multiplier\n",
        "        return alpha / (2.0 * noise_multiplier**2)\n",
        "    if noise_multiplier == 0:\n",
        "         return float('inf') # Infinite privacy cost if noise is zero and sampled\n",
        "\n",
        "    sigma_squared = noise_multiplier**2\n",
        "\n",
        "    # Compute the sum using log-sum-exp trick for numerical stability\n",
        "    log_sum_exp = -float('inf')\n",
        "    log_q = math.log(q)\n",
        "    log_1_minus_q = math.log1p(-q) # More accurate for small q\n",
        "\n",
        "    for k in range(alpha + 1):\n",
        "        log_comb_term = _log_comb(alpha, k)\n",
        "        if log_comb_term == -float('inf'):\n",
        "            continue\n",
        "\n",
        "        # Term involving probabilities: k * log(q) + (alpha - k) * log(1-q)\n",
        "        log_prob_term = k * log_q + (alpha - k) * log_1_minus_q\n",
        "\n",
        "        # Term involving the RDP of non-sampled mechanism at order k\n",
        "        # We need exp((k-1) * rdp_epsilon(k)) = exp((k-1) * k / (2 * sigma^2))\n",
        "        # Handle k=0 and k=1 where the exponent term is 0 -> exp(0) = 1 -> log(1) = 0\n",
        "        log_exp_term = 0.0\n",
        "        if k > 1:\n",
        "            log_exp_term = (k - 1.0) * k / (2.0 * sigma_squared)\n",
        "\n",
        "        # Combine terms in log space: log(C(a,k) * q^k * (1-q)^(a-k) * exp(...))\n",
        "        current_term_log = log_comb_term + log_prob_term + log_exp_term\n",
        "\n",
        "        # Add to the total sum using log-add-exp\n",
        "        log_sum_exp = _log_add_exp(log_sum_exp, current_term_log)\n",
        "\n",
        "    # Final RDP epsilon is log(sum) / (alpha - 1)\n",
        "    rdp_epsilon = log_sum_exp / (alpha - 1.0)\n",
        "\n",
        "    return rdp_epsilon\n",
        "\n",
        "class LaplaceAccumulator:\n",
        "    \"\"\"Simple pure-DP counter for Laplace ε's.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.eps_sum = 0.0\n",
        "        self._eps_history = []  # Track epsilon history\n",
        "\n",
        "    def step(self, eps: float):\n",
        "        assert eps >= 0\n",
        "        self.eps_sum += eps\n",
        "        self._eps_history.append(self.eps_sum)\n",
        "\n",
        "class ManualPrivacyAccountant:\n",
        "    \"\"\"\n",
        "    Manually implemented Moments Accountant (based on Renyi DP) to track\n",
        "    cumulative privacy cost (epsilon, delta) for the Gaussian Noise mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, moment_orders=None):\n",
        "        \"\"\"\n",
        "        Initializes the accountant.\n",
        "\n",
        "        Args:\n",
        "            moment_orders: A list of RDP orders (alpha values > 1) to track.\n",
        "                           If None, uses a default set.\n",
        "        \"\"\"\n",
        "        # Use a default set of orders if none provided\n",
        "        if moment_orders is None:\n",
        "            moment_orders = list(range(2, 33)) + [40.0, 48.0, 56.0, 64.0]\n",
        "            moment_orders = [int(a) for a in moment_orders if isinstance(a, (int, float)) and a > 1]\n",
        "            moment_orders = sorted(list(set(moment_orders))) # Unique & sorted\n",
        "\n",
        "        if not moment_orders or any(alpha <= 1 for alpha in moment_orders):\n",
        "            raise ValueError(\"Moment orders (alphas) must be > 1.\")\n",
        "\n",
        "        self.moment_orders = moment_orders\n",
        "        # Store total accumulated RDP epsilon for each order alpha\n",
        "        self._total_rdp_epsilons = {alpha: 0.0 for alpha in self.moment_orders}\n",
        "        self._steps = 0 # Track total number of steps taken\n",
        "\n",
        "    def step(self, noise_multiplier: float, sampling_rate: float, num_steps: int = 1):\n",
        "        \"\"\"\n",
        "        Records the privacy cost of applying the sampled Gaussian mechanism\n",
        "        for a number of steps.\n",
        "        \"\"\"\n",
        "        if noise_multiplier < 0:\n",
        "            raise ValueError(\"Noise multiplier cannot be negative.\")\n",
        "        if not (0 <= sampling_rate <= 1):\n",
        "            raise ValueError(\"Sampling rate must be between 0 and 1.\")\n",
        "        if num_steps <= 0:\n",
        "            return # No steps taken\n",
        "\n",
        "        for alpha in self.moment_orders:\n",
        "            # Calculate RDP epsilon for a *single* step with these params\n",
        "            # Ensure alpha is int for _compute_rdp_epsilon_step as implemented\n",
        "            rdp_epsilon_step = _compute_rdp_epsilon_step(sampling_rate, noise_multiplier, int(alpha))\n",
        "\n",
        "            # Accumulate the total RDP epsilon for this order\n",
        "            self._total_rdp_epsilons[alpha] += num_steps * rdp_epsilon_step\n",
        "\n",
        "        self._steps += num_steps\n",
        "\n",
        "    def get_privacy_spent(self, delta: float) -> tuple:\n",
        "        \"\"\"\n",
        "        Computes the (epsilon, delta)-DP guarantee for the accumulated\n",
        "        privacy cost.\n",
        "        \"\"\"\n",
        "        if delta <= 0:\n",
        "            print(\"Warning: Target delta must be positive.\")\n",
        "            return float('inf'), delta\n",
        "\n",
        "        min_epsilon = float('inf')\n",
        "\n",
        "        for alpha in self.moment_orders:\n",
        "            total_rdp_epsilon = self._total_rdp_epsilons[alpha]\n",
        "\n",
        "            if total_rdp_epsilon == float('inf'):\n",
        "                continue # This alpha gives infinite epsilon\n",
        "\n",
        "            # Formula to convert RDP epsilon(alpha) to (epsilon, delta)-DP:\n",
        "            # epsilon = RDP_epsilon(alpha) - log(delta) / (alpha - 1)\n",
        "            epsilon = total_rdp_epsilon - (math.log(delta) / (alpha - 1.0))\n",
        "\n",
        "            # Ensure epsilon is not negative\n",
        "            epsilon = max(0.0, epsilon)\n",
        "\n",
        "            min_epsilon = min(min_epsilon, epsilon)\n",
        "\n",
        "        return min_epsilon, delta\n",
        "\n",
        "    @property\n",
        "    def total_steps(self):\n",
        "        return self._steps\n",
        "\n",
        "class HybridAccountant:\n",
        "    \"\"\"\n",
        "    Tracks:\n",
        "      1) pure-DP ε from Laplace steps,\n",
        "      2) RDP from Gaussian steps via ManualPrivacyAccountant.\n",
        "    \"\"\"\n",
        "    def __init__(self, noise_multiplier, sampling_rate, moment_orders=None):\n",
        "        # for Gaussian RDP\n",
        "        self.gauss_acc = ManualPrivacyAccountant(moment_orders)\n",
        "        self.fixed_sigma = noise_multiplier\n",
        "        self.q = sampling_rate\n",
        "        self._eps_history = []  # Track total epsilon history\n",
        "\n",
        "        # for Laplace pure-DP\n",
        "        self.laplace_acc = LaplaceAccumulator()\n",
        "\n",
        "    def laplace_step(self, epsilon_prime: float):\n",
        "        \"\"\"Call every time you inject Laplace(…,ε′).\"\"\"\n",
        "        self.laplace_acc.step(epsilon_prime)\n",
        "        # Update the combined history\n",
        "        self._update_history()\n",
        "\n",
        "    def gaussian_step(self, noise_multiplier=None, num_steps: int = 1):\n",
        "        \"\"\"Call every time you inject Gaussian noise on gradients.\"\"\"\n",
        "        # uses the *fixed* sigma for all steps (unless adaptive is provided)\n",
        "        sigma = noise_multiplier if noise_multiplier is not None else self.fixed_sigma\n",
        "        self.gauss_acc.step(sigma, self.q, num_steps)\n",
        "        # Update the combined history\n",
        "        self._update_history()\n",
        "\n",
        "    def _update_history(self):\n",
        "        \"\"\"Update the combined epsilon history.\"\"\"\n",
        "        eps_gauss, _ = self.gauss_acc.get_privacy_spent(delta=1e-5)\n",
        "        eps_lap = self.laplace_acc.eps_sum\n",
        "        self._eps_history.append(eps_lap + eps_gauss)\n",
        "\n",
        "    def get_privacy_spent(self, delta: float):\n",
        "        \"\"\"\n",
        "        Returns the composed (ε,δ):\n",
        "          ε = ε_laplace + ε_gauss\n",
        "          δ = δ  (pure-DP from Laplace contributes no δ)\n",
        "        \"\"\"\n",
        "        eps_gauss, _ = self.gauss_acc.get_privacy_spent(delta)\n",
        "        eps_lap = self.laplace_acc.eps_sum\n",
        "        return eps_lap + eps_gauss, delta\n",
        "        \n",
        "    @property\n",
        "    def epsilon_laplace(self):\n",
        "        \"\"\"Get the current Laplace privacy cost.\"\"\"\n",
        "        return self.laplace_acc.eps_sum\n",
        "        \n",
        "    @property\n",
        "    def epsilon_gaussian(self):\n",
        "        \"\"\"Get the current Gaussian privacy cost.\"\"\"\n",
        "        eps_gauss, _ = self.gauss_acc.get_privacy_spent(delta=1e-5)\n",
        "        return eps_gauss\n",
        "        \n",
        "    @property\n",
        "    def total_steps(self):\n",
        "        \"\"\"Get the total number of Gaussian steps.\"\"\"\n",
        "        return self.gauss_acc.total_steps\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Split Federated Learning Components\n",
        "\n",
        "Now we'll implement the core components of Split Federated Learning:\n",
        "1. Client class for local computations\n",
        "2. Server class for handling activation gradients\n",
        "3. Federation server for coordinating updates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split Federated Learning implementation\n",
        "\n",
        "class SFLClient:\n",
        "    \"\"\"\n",
        "    Client in Split Federated Learning.\n",
        "    Manages local data, client-side model (WC), performs local computations,\n",
        "    applies noise, and communicates intermediate results.\n",
        "    \"\"\"\n",
        "    def __init__(self, client_id: int, client_model: nn.Module, dataloader: DataLoader, config: dict, device: torch.device):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            client_id: Unique identifier for the client.\n",
        "            client_model: A *copy* of the initial client-side model (WC) architecture.\n",
        "            dataloader: DataLoader for the client's local dataset partition.\n",
        "            config: Configuration dictionary.\n",
        "            device: The torch device ('cpu' or 'cuda').\n",
        "        \"\"\"\n",
        "        self.client_id = client_id\n",
        "        self.client_model = client_model.to(device)\n",
        "        self.dataloader = dataloader\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.optimizer = self._create_optimizer() # Optimizer for WC\n",
        "\n",
        "        # Store intermediate activation for backward pass\n",
        "        self._activations = None\n",
        "        self._data_batch = None # Store data batch to access individual samples for clipping\n",
        "        self._labels_batch = None # Store labels corresponding to the data batch\n",
        "\n",
        "        # Adaptive DP: Store previous round's gradient norms and current noise scale\n",
        "        self._prev_round_grad_norms = None\n",
        "        self._current_clip_threshold = config['dp_noise']['clip_norm'] # Initial threshold\n",
        "        self._current_sigma = config['dp_noise']['initial_sigma'] # Initial noise scale\n",
        "\n",
        "    def _create_optimizer(self) -> optim.Optimizer:\n",
        "        \"\"\"Creates the optimizer for the client-side model (WC).\"\"\"\n",
        "        lr = self.config.get('lr', 0.01)\n",
        "        optimizer_name = self.config.get('optimizer', 'SGD').lower()\n",
        "        if optimizer_name == 'sgd':\n",
        "            return optim.SGD(self.client_model.parameters(), lr=lr)\n",
        "        elif optimizer_name == 'adam':\n",
        "            return optim.Adam(self.client_model.parameters(), lr=lr)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
        "\n",
        "    def set_model_params(self, global_params: OrderedDict):\n",
        "        \"\"\"Updates the local client model (WC) with parameters from the FedServer.\"\"\"\n",
        "        self.client_model.load_state_dict(global_params)\n",
        "\n",
        "    def update_noise_scale(self, new_sigma: float):\n",
        "        \"\"\"Updates the current noise scale sigma_t.\"\"\"\n",
        "        self._current_sigma = new_sigma\n",
        "\n",
        "    def _calculate_adaptive_clip_threshold(self) -> float:\n",
        "        \"\"\"Calculates the adaptive clipping threshold Ck_t for the current round.\"\"\"\n",
        "        # Check if adaptive clipping is disabled (adaptive_clipping_factor = 0.0)\n",
        "        if self.config['dp_noise']['adaptive_clipping_factor'] == 0.0:\n",
        "            # For fixed DP, always use the initial clip norm\n",
        "            return self.config['dp_noise']['clip_norm']\n",
        "            \n",
        "        if self._prev_round_grad_norms is None:\n",
        "            # First round: use initial threshold\n",
        "            return self.config['dp_noise']['clip_norm']\n",
        "        \n",
        "        # Calculate mean norm from previous round\n",
        "        mean_norm = torch.mean(torch.tensor(self._prev_round_grad_norms, device=self.device))\n",
        "        # Apply adaptive factor\n",
        "        adaptive_factor = self.config['dp_noise']['adaptive_clipping_factor']\n",
        "        return float(adaptive_factor * mean_norm)\n",
        "\n",
        "    def local_forward_pass(self) -> tuple:\n",
        "        \"\"\"\n",
        "        Performs the forward pass on the client model (WC) using one batch of local data.\n",
        "        Applies Laplacian noise to the activations before returning.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            data, labels = next(iter(self.dataloader))\n",
        "        except StopIteration:\n",
        "            print(f\"Client {self.client_id}: Dataloader exhausted. Re-initializing for simulation.\")\n",
        "            self.dataloader = DataLoader(self.dataloader.dataset, batch_size=self.config['batch_size'], shuffle=True)\n",
        "            data, labels = next(iter(self.dataloader))\n",
        "\n",
        "        data, labels = data.to(self.device), labels.to(self.device)\n",
        "        self._data_batch = data\n",
        "        self._labels_batch = labels\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        activations = self.client_model(data)\n",
        "\n",
        "        # Laplacian Noise (Mechanism 1)\n",
        "        sensitivity = self.config['dp_noise']['laplacian_sensitivity']\n",
        "        epsilon_prime = self.config['dp_noise']['epsilon_prime']\n",
        "\n",
        "        if sensitivity > 0:\n",
        "            noisy_activations = add_laplacian_noise(\n",
        "                activations,\n",
        "                sensitivity,\n",
        "                epsilon_prime,\n",
        "                device=self.device\n",
        "            )\n",
        "        else:\n",
        "            noisy_activations = activations\n",
        "\n",
        "        self._activations_for_backward = activations  # Store the original activations for backward pass\n",
        "        return noisy_activations.detach().clone(), labels.clone()\n",
        "\n",
        "    def local_backward_pass(self, activation_grads: torch.Tensor) -> dict:\n",
        "        \"\"\"\n",
        "        Performs the backward pass with adaptive clipping and noise.\n",
        "        \"\"\"\n",
        "        if self._activations_for_backward is None or self._data_batch is None:\n",
        "            raise RuntimeError(\"Client must perform forward pass before backward pass.\")\n",
        "\n",
        "        # Calculate adaptive clipping threshold for this round\n",
        "        self._current_clip_threshold = self._calculate_adaptive_clip_threshold()\n",
        "        \n",
        "        summed_clipped_grads = OrderedDict([(name, torch.zeros_like(param)) \n",
        "                                          for name, param in self.client_model.named_parameters() \n",
        "                                          if param.requires_grad])\n",
        "        \n",
        "        batch_size = self._data_batch.size(0)\n",
        "        activation_grads = activation_grads.to(self.device)\n",
        "        current_round_grad_norms = [] # Store norms for next round's threshold calculation\n",
        "\n",
        "        # Per-sample gradient computation with adaptive clipping\n",
        "        for i in range(batch_size):\n",
        "            self.optimizer.zero_grad()\n",
        "            sample_activation = self._activations_for_backward[i:i+1]\n",
        "            sample_activation_grad = activation_grads[i:i+1]\n",
        "            \n",
        "            sample_activation.backward(gradient=sample_activation_grad, retain_graph=True)\n",
        "            \n",
        "            # Calculate L2 norm of gradients for this sample\n",
        "            total_norm_sq = torch.zeros(1, device=self.device)\n",
        "            for name, param in self.client_model.named_parameters():\n",
        "                if param.grad is not None:\n",
        "                    total_norm_sq += param.grad.norm(2).item() ** 2\n",
        "            \n",
        "            total_norm = torch.sqrt(total_norm_sq)\n",
        "            current_round_grad_norms.append(total_norm.item())\n",
        "            \n",
        "            # Clip gradients using adaptive threshold\n",
        "            clip_coef = min(1.0, self._current_clip_threshold / (total_norm + 1e-6))\n",
        "            \n",
        "            for name, param in self.client_model.named_parameters():\n",
        "                if param.grad is not None:\n",
        "                    summed_clipped_grads[name] += param.grad.data * clip_coef\n",
        "\n",
        "        # Store gradient norms for next round's threshold calculation\n",
        "        self._prev_round_grad_norms = current_round_grad_norms\n",
        "\n",
        "        # Add Gaussian noise with adaptive scale\n",
        "        noisy_gradients = OrderedDict()\n",
        "        if self._current_sigma > 0:\n",
        "            for name, summed_grad in summed_clipped_grads.items():\n",
        "                noisy_gradients[name] = add_gaussian_noise(\n",
        "                    summed_grad,\n",
        "                    self._current_clip_threshold,\n",
        "                    self._current_sigma,\n",
        "                    device=self.device\n",
        "                )\n",
        "        else:\n",
        "            noisy_gradients = summed_clipped_grads\n",
        "\n",
        "        # Clear intermediate values\n",
        "        self._activations_for_backward = None\n",
        "        self._data_batch = None\n",
        "        self._labels_batch = None\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        return noisy_gradients\n",
        "\n",
        "class MainServer:\n",
        "    \"\"\"\n",
        "    Main Server in Split Federated Learning.\n",
        "    Manages the server-side model (WS), processes client activations,\n",
        "    computes gradients for clients, and updates the server model.\n",
        "    \"\"\"\n",
        "    def __init__(self, server_model: nn.Module, config: dict, device: torch.device):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            server_model: The server-side model (WS) architecture.\n",
        "            config: Configuration dictionary.\n",
        "            device: The torch device ('cpu' or 'cuda').\n",
        "        \"\"\"\n",
        "        self.server_model = server_model.to(device)\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.optimizer = self._create_optimizer()\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "        # Store client data\n",
        "        self.client_data = {}  # {client_id: (activations, labels)}\n",
        "        self.activation_gradients = {}  # {client_id: gradients}\n",
        "        \n",
        "    def _create_optimizer(self) -> optim.Optimizer:\n",
        "        \"\"\"Creates the optimizer for the server-side model (WS).\"\"\"\n",
        "        lr = self.config.get('lr', 0.01)\n",
        "        optimizer_name = self.config.get('optimizer', 'SGD').lower()\n",
        "        if optimizer_name == 'sgd':\n",
        "            return optim.SGD(self.server_model.parameters(), lr=lr)\n",
        "        elif optimizer_name == 'adam':\n",
        "            return optim.Adam(self.server_model.parameters(), lr=lr)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
        "    \n",
        "    def receive_client_data(self, client_id: int, activations: torch.Tensor, labels: torch.Tensor):\n",
        "        \"\"\"Stores client activations and labels for processing.\"\"\"\n",
        "        self.client_data[client_id] = (activations.to(self.device), labels.to(self.device))\n",
        "    \n",
        "    def clear_round_data(self):\n",
        "        \"\"\"Clears all client data from the current round.\"\"\"\n",
        "        self.client_data = {}\n",
        "        self.activation_gradients = {}\n",
        "    \n",
        "    def forward_backward_pass(self, client_id: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs forward and backward pass on the server model for a client.\n",
        "        \n",
        "        Args:\n",
        "            client_id: The ID of the client whose data to process.\n",
        "            \n",
        "        Returns:\n",
        "            The gradient of the activations.\n",
        "        \"\"\"\n",
        "        if client_id not in self.client_data:\n",
        "            raise ValueError(f\"No data received from client {client_id}\")\n",
        "        \n",
        "        activations, labels = self.client_data[client_id]\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = self.server_model(activations)\n",
        "        loss = self.criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # Store and return activation gradients\n",
        "        act_grad = activations.grad.clone()\n",
        "        self.activation_gradients[client_id] = act_grad\n",
        "        \n",
        "        return act_grad\n",
        "    \n",
        "    def aggregate_and_update(self):\n",
        "        \"\"\"Aggregates losses and updates the server model.\"\"\"\n",
        "        # Compute average loss if we have client data\n",
        "        if not self.client_data:\n",
        "            return 0.0\n",
        "        \n",
        "        # Since the backward passes have already populated the gradients,\n",
        "        # we just need to step the optimizer\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return self.server_model.parameters()\n",
        "    \n",
        "    def get_server_model(self) -> nn.Module:\n",
        "        \"\"\"Returns the current server model.\"\"\"\n",
        "        return self.server_model\n",
        "    \n",
        "    def get_criterion(self):\n",
        "        \"\"\"Returns the loss criterion used by the server.\"\"\"\n",
        "        return self.criterion\n",
        "\n",
        "class FedServer:\n",
        "    \"\"\"\n",
        "    Federation Server in Split Federated Learning.\n",
        "    Manages the global client-side model (WC) and aggregates client updates.\n",
        "    \"\"\"\n",
        "    def __init__(self, client_model: nn.Module, config: dict, device: torch.device):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            client_model: An instance of the client-side model (WC) architecture.\n",
        "            config: Configuration dictionary.\n",
        "            device: The torch device ('cpu' or 'cuda').\n",
        "        \"\"\"\n",
        "        self.client_model = client_model.to(device)\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.optimizer = self._create_optimizer()\n",
        "        self._client_updates = []  # Stores received client updates in a round\n",
        "\n",
        "        # Adaptive DP: Track validation loss and noise scale\n",
        "        self._current_sigma = config['dp_noise']['initial_sigma']\n",
        "        self._validation_losses = []  # Store recent validation losses\n",
        "        self._noise_decay_patience = config['dp_noise']['noise_decay_patience']\n",
        "        self._adaptive_noise_decay_factor = config['dp_noise']['adaptive_noise_decay_factor']\n",
        "        self._criterion = nn.CrossEntropyLoss()\n",
        "        self._sigma_history = [self._current_sigma]  # Track sigma history\n",
        "\n",
        "    def _create_optimizer(self) -> optim.Optimizer:\n",
        "        \"\"\"Creates the optimizer for the global client-side model (WC).\"\"\"\n",
        "        lr = self.config.get('lr', 0.01)\n",
        "        optimizer_name = self.config.get('optimizer', 'SGD').lower()\n",
        "\n",
        "        if optimizer_name == 'sgd':\n",
        "            return optim.SGD(self.client_model.parameters(), lr=lr)\n",
        "        elif optimizer_name == 'adam':\n",
        "            return optim.Adam(self.client_model.parameters(), lr=lr)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
        "\n",
        "    def get_client_model_params(self) -> OrderedDict:\n",
        "        \"\"\"Returns the state dictionary of the current global client model (WC).\"\"\"\n",
        "        return self.client_model.state_dict()\n",
        "\n",
        "    def receive_client_update(self, client_update: dict):\n",
        "        \"\"\"\n",
        "        Receives and stores an update (typically gradients) from a client.\n",
        "        \"\"\"\n",
        "        # Ensure updates are on the correct device and detached\n",
        "        processed_update = OrderedDict()\n",
        "        for name, param in client_update.items():\n",
        "            processed_update[name] = param.detach().clone().to(self.device)\n",
        "        self._client_updates.append(processed_update)\n",
        "\n",
        "    def _update_noise_scale(self, validation_loss: float):\n",
        "        \"\"\"\n",
        "        Updates the noise scale based on validation loss trend.\n",
        "        \"\"\"\n",
        "        self._validation_losses.append(validation_loss)\n",
        "        \n",
        "        # Check if we have enough history to make a decision\n",
        "        if len(self._validation_losses) < self._noise_decay_patience + 1:\n",
        "            return\n",
        "        \n",
        "        # Check if loss has been decreasing for the required number of rounds\n",
        "        recent_losses = self._validation_losses[-self._noise_decay_patience:]\n",
        "        is_decreasing = all(recent_losses[i] > recent_losses[i+1] \n",
        "                          for i in range(len(recent_losses)-1))\n",
        "        \n",
        "        if is_decreasing:\n",
        "            # Decrease noise scale\n",
        "            self._current_sigma *= self._adaptive_noise_decay_factor\n",
        "            print(f\"FedServer: Loss decreasing for {self._noise_decay_patience} rounds. \"\n",
        "                  f\"Updated noise scale to {self._current_sigma:.4f}\")\n",
        "            # Track sigma change\n",
        "            self._sigma_history.append(self._current_sigma)\n",
        "\n",
        "    def get_current_sigma(self) -> float:\n",
        "        \"\"\"Returns the current noise scale sigma_t.\"\"\"\n",
        "        return self._current_sigma\n",
        "\n",
        "    def aggregate_updates(self, validation_loader=None, main_server=None):\n",
        "        \"\"\"\n",
        "        Aggregates the received client updates using FedAvg and updates the global client model (WC).\n",
        "        \"\"\"\n",
        "        if not self._client_updates:\n",
        "            print(\"FedServer: No client updates received for aggregation.\")\n",
        "            return\n",
        "\n",
        "        # Federated averaging for gradients\n",
        "        averaged_gradients = self.federated_averaging_gradients(self._client_updates)\n",
        "\n",
        "        # Update the global client model parameters using the averaged gradients via the optimizer\n",
        "        self.optimizer.zero_grad()\n",
        "        with torch.no_grad():  # Manually assign gradients\n",
        "            for name, param in self.client_model.named_parameters():\n",
        "                if name in averaged_gradients:\n",
        "                    if param.grad is None:\n",
        "                        param.grad = torch.zeros_like(param)\n",
        "                    param.grad.copy_(averaged_gradients[name])\n",
        "                else:\n",
        "                    if param.grad is not None:\n",
        "                        param.grad.zero_()\n",
        "\n",
        "        self.optimizer.step()  # Update parameters using assigned gradients\n",
        "\n",
        "        # Update noise scale and evaluate metrics if validation loader is provided\n",
        "        if validation_loader is not None:\n",
        "            validation_loss, accuracy = self.evaluate_metrics(validation_loader, main_server)\n",
        "            self._update_noise_scale(validation_loss)\n",
        "\n",
        "        # Clear updates for the next round\n",
        "        self._client_updates = []\n",
        "\n",
        "    def evaluate_metrics(self, validation_loader, main_server=None) -> tuple:\n",
        "        \"\"\"\n",
        "        Evaluates and prints both validation loss and accuracy.\n",
        "        \"\"\"\n",
        "        self.client_model.eval()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        # Check if we can evaluate complete model\n",
        "        evaluate_complete = main_server is not None\n",
        "        \n",
        "        if evaluate_complete:\n",
        "            # Get server model\n",
        "            server_model = main_server.get_server_model()\n",
        "            server_model.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for data, labels in validation_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                \n",
        "                # Forward pass through client model\n",
        "                client_outputs = self.client_model(data)\n",
        "                \n",
        "                if evaluate_complete:\n",
        "                    # Complete forward pass through server model\n",
        "                    outputs = server_model(client_outputs)\n",
        "                    loss = main_server.get_criterion()(outputs, labels)\n",
        "                    \n",
        "                    # Calculate accuracy\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                else:\n",
        "                    # Just use client outputs (which aren't actual predictions)\n",
        "                    outputs = client_outputs\n",
        "                    loss = self._criterion(outputs, labels)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                total_loss += loss.item()\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        avg_loss = total_loss / len(validation_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "        \n",
        "        print(f\"\\nValidation Metrics:\")\n",
        "        if evaluate_complete:\n",
        "            print(f\"Complete Model - Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
        "        else:\n",
        "            print(f\"Client Model Only (incomplete) - Loss: {avg_loss:.4f}\")\n",
        "        \n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def federated_averaging_gradients(self, gradients_list):\n",
        "        \"\"\"\n",
        "        Implements FedAvg for a list of gradients.\n",
        "        \"\"\"\n",
        "        # Initialize with zeros\n",
        "        avg_grad = OrderedDict()\n",
        "        \n",
        "        # Get the first client's gradients to determine the structure\n",
        "        for name, grad in gradients_list[0].items():\n",
        "            avg_grad[name] = torch.zeros_like(grad)\n",
        "            \n",
        "        # Sum all gradients\n",
        "        for gradients in gradients_list:\n",
        "            for name, grad in gradients.items():\n",
        "                avg_grad[name] += grad\n",
        "                \n",
        "        # Divide by the number of clients to get the average\n",
        "        for name in avg_grad.keys():\n",
        "            avg_grad[name] = avg_grad[name] / len(gradients_list)\n",
        "            \n",
        "        return avg_grad\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Training Loop and Experiment Configurations\n",
        "\n",
        "Now we'll define the training loop and experiment configurations for our comparative study.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Default configuration for experiments\n",
        "default_config = {\n",
        "    'batch_size': 32,\n",
        "    'optimizer': 'adam',\n",
        "    'lr': 0.001,\n",
        "    'num_rounds': 50,\n",
        "    'num_clients': 5,\n",
        "    'log_interval': 5,\n",
        "    'split_layer': 1,  # Layer index at which to split the model\n",
        "    'seed': 42,\n",
        "    'dp_noise': {\n",
        "        'mode': 'hybrid',  # 'vanilla', 'adaptive', or 'hybrid'\n",
        "        'initial_sigma': 1.0,  # Initial noise multiplier for Gaussian\n",
        "        'clip_norm': 1.0,  # Initial gradient clipping norm\n",
        "        'adaptive_clipping_factor': 1.5,  # For adaptive clipping (C_t = factor * mean_norm)\n",
        "        'adaptive_noise_decay_factor': 0.9,  # Decay factor for sigma when validation loss decreases\n",
        "        'noise_decay_patience': 3,  # Number of rounds with decreasing loss before decaying sigma\n",
        "        'delta': 1e-5,  # Target delta for privacy accounting\n",
        "        'laplacian_sensitivity': 0.1,  # Sensitivity for Laplacian noise on activations\n",
        "        'epsilon_prime': 0.1,  # Privacy budget for Laplacian mechanism\n",
        "        'validation_set_ratio': 0.1  # Ratio of training data to use for validation\n",
        "    }\n",
        "}\n",
        "\n",
        "def create_experiment_configs():\n",
        "    \"\"\"Create different experiment configurations for comparison.\"\"\"\n",
        "    \n",
        "    configs = []\n",
        "    \n",
        "    # Base configuration combinations\n",
        "    datasets = ['mnist', 'fashion_mnist', 'cifar10']\n",
        "    models = {\n",
        "        'mnist': 'simple_cnn',\n",
        "        'fashion_mnist': 'simple_cnn',\n",
        "        'cifar10': 'cifar_cnn'\n",
        "    }\n",
        "    dp_modes = ['vanilla', 'adaptive']\n",
        "    distributions = ['iid', 'dirichlet']\n",
        "    alphas = [0.1, 0.5, 1.0, 10.0]  # Dirichlet concentration parameters\n",
        "    client_counts = [5, 10, 20]\n",
        "    \n",
        "    # Generate combinations\n",
        "    for dataset in datasets:\n",
        "        model = models[dataset]\n",
        "        for dp_mode in dp_modes:\n",
        "            for distribution in distributions:\n",
        "                if distribution == 'dirichlet':\n",
        "                    for alpha in alphas:\n",
        "                        for num_clients in client_counts:\n",
        "                            # Create config variation\n",
        "                            config = copy.deepcopy(default_config)\n",
        "                            config['dataset'] = dataset\n",
        "                            config['model'] = model\n",
        "                            config['dp_noise']['mode'] = dp_mode\n",
        "                            config['partition_method'] = distribution\n",
        "                            config['dirichlet_alpha'] = alpha\n",
        "                            config['num_clients'] = num_clients\n",
        "                            \n",
        "                            # Create a unique identifier for this experiment\n",
        "                            config['id'] = f\"{dataset}_{model}_{dp_mode}_{distribution}_alpha{alpha}_clients{num_clients}\"\n",
        "                            \n",
        "                            configs.append(config)\n",
        "                else:  # IID distribution\n",
        "                    for num_clients in client_counts:\n",
        "                        # Create config variation\n",
        "                        config = copy.deepcopy(default_config)\n",
        "                        config['dataset'] = dataset\n",
        "                        config['model'] = model\n",
        "                        config['dp_noise']['mode'] = dp_mode\n",
        "                        config['partition_method'] = distribution\n",
        "                        config['num_clients'] = num_clients\n",
        "                        \n",
        "                        # Create a unique identifier for this experiment\n",
        "                        config['id'] = f\"{dataset}_{model}_{dp_mode}_{distribution}_clients{num_clients}\"\n",
        "                        \n",
        "                        configs.append(config)\n",
        "    \n",
        "    return configs\n",
        "\n",
        "# Generate experiment configurations\n",
        "experiment_configs = create_experiment_configs()\n",
        "print(f\"Generated {len(experiment_configs)} experiment configurations.\")\n",
        "\n",
        "# Show a sample configuration\n",
        "sample_config = experiment_configs[0]\n",
        "print(\"\\nSample experiment configuration:\")\n",
        "for key, value in sample_config.items():\n",
        "    if key != 'dp_noise':  # Don't print the nested dp_noise dictionary\n",
        "        print(f\"{key}: {value}\")\n",
        "print(\"\\nDP Noise settings:\")\n",
        "for key, value in sample_config['dp_noise'].items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_sfl(config, results_dict=None):\n",
        "    \"\"\"\n",
        "    Train a model using Split Federated Learning.\n",
        "    \n",
        "    Args:\n",
        "        config: Configuration dictionary\n",
        "        results_dict: Optional dictionary to store results\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    set_seed(config['seed'])\n",
        "    \n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Load dataset\n",
        "    train_dataset, test_dataset = load_dataset(config['dataset'])\n",
        "    \n",
        "    # Split training data into client data and validation set\n",
        "    validation_ratio = config['dp_noise']['validation_set_ratio']\n",
        "    validation_size = int(len(train_dataset) * validation_ratio)\n",
        "    train_size = len(train_dataset) - validation_size\n",
        "    \n",
        "    train_subset, validation_subset = torch.utils.data.random_split(\n",
        "        train_dataset, [train_size, validation_size]\n",
        "    )\n",
        "    \n",
        "    validation_loader = torch.utils.data.DataLoader(\n",
        "        validation_subset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False,\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "    \n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False,\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "    \n",
        "    # Create client dataloaders\n",
        "    distribution = config.get('partition_method', 'iid')\n",
        "    alpha = config.get('dirichlet_alpha', 1.0)  # Only used for dirichlet distribution\n",
        "    \n",
        "    client_loaders = get_client_dataloaders(\n",
        "        train_subset, \n",
        "        config['num_clients'], \n",
        "        config['batch_size'],\n",
        "        distribution=distribution,\n",
        "        alpha=alpha\n",
        "    )\n",
        "    \n",
        "    # Initialize model\n",
        "    full_model = get_model(config['model'], config['dataset'], config['split_layer'])\n",
        "    \n",
        "    # Split model\n",
        "    client_model = nn.ModuleList()\n",
        "    server_model = nn.ModuleList()\n",
        "    \n",
        "    # Extract client and server parts\n",
        "    for i, layer in enumerate(full_model.client_layers):\n",
        "        if i < config['split_layer']:\n",
        "            client_model.append(layer)\n",
        "    \n",
        "    for layer in full_model.server_layers:\n",
        "        server_model.append(layer)\n",
        "    \n",
        "    # Wrap in nn.Sequential\n",
        "    client_model = nn.Sequential(*client_model)\n",
        "    server_model = nn.Sequential(*server_model)\n",
        "    \n",
        "    # Create FedServer and MainServer\n",
        "    fed_server = FedServer(client_model, config, device)\n",
        "    main_server = MainServer(server_model, config, device)\n",
        "    \n",
        "    # Create clients\n",
        "    clients = []\n",
        "    for i in range(config['num_clients']):\n",
        "        client_model_copy = copy.deepcopy(client_model)\n",
        "        client = SFLClient(\n",
        "            client_id=i, \n",
        "            client_model=client_model_copy, \n",
        "            dataloader=client_loaders[i], \n",
        "            config=config, \n",
        "            device=device\n",
        "        )\n",
        "        clients.append(client)\n",
        "    \n",
        "    # Privacy accountant\n",
        "    sampling_rate = config['batch_size'] / train_size\n",
        "    privacy_accountant = HybridAccountant(\n",
        "        noise_multiplier=config['dp_noise']['initial_sigma'],\n",
        "        sampling_rate=sampling_rate\n",
        "    )\n",
        "    \n",
        "    # Training metrics\n",
        "    metrics = {\n",
        "        'train_loss': [],\n",
        "        'validation_loss': [],\n",
        "        'test_accuracy': [],\n",
        "        'epsilon': [],\n",
        "        'epsilon_laplace': [],\n",
        "        'epsilon_gaussian': [],\n",
        "        'sigma': [],\n",
        "        'round_times': []\n",
        "    }\n",
        "    \n",
        "    # Training loop\n",
        "    num_rounds = config['num_rounds']\n",
        "    log_interval = config['log_interval']\n",
        "    \n",
        "    # Initial test accuracy\n",
        "    test_accuracy = evaluate_model(full_model, test_loader, device)\n",
        "    metrics['test_accuracy'].append(test_accuracy)\n",
        "    print(f\"Initial Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    \n",
        "    for round_num in tqdm(range(num_rounds), desc=\"Training Rounds\"):\n",
        "        round_start_time = time.time()\n",
        "        \n",
        "        # Broadcast current noise scale to clients\n",
        "        current_sigma = fed_server.get_current_sigma()\n",
        "        for client in clients:\n",
        "            client.update_noise_scale(current_sigma)\n",
        "        \n",
        "        # Broadcast client model parameters\n",
        "        global_client_params = fed_server.get_client_model_params()\n",
        "        for client in clients:\n",
        "            client.set_model_params(global_client_params)\n",
        "        \n",
        "        # Clear previous round data\n",
        "        client_activation_grads = {}\n",
        "        client_noisy_wc_grads = []\n",
        "        main_server.clear_round_data()\n",
        "        \n",
        "        # Client forward passes\n",
        "        client_data_for_main_server = {}\n",
        "        for client in clients:\n",
        "            noisy_activations, labels = client.local_forward_pass()\n",
        "            client_data_for_main_server[client.client_id] = (noisy_activations, labels)\n",
        "            \n",
        "            # Track Laplace privacy cost\n",
        "            if config['dp_noise']['laplacian_sensitivity'] > 0:\n",
        "                privacy_accountant.laplace_step(config['dp_noise']['epsilon_prime'])\n",
        "        \n",
        "        # Server processes client data\n",
        "        for client_id, (noisy_acts, lbls) in client_data_for_main_server.items():\n",
        "            main_server.receive_client_data(client_id, noisy_acts, lbls)\n",
        "        \n",
        "        # Server computes activation gradients\n",
        "        for client_id in client_data_for_main_server.keys():\n",
        "            act_grad = main_server.forward_backward_pass(client_id)\n",
        "            client_activation_grads[client_id] = act_grad\n",
        "        \n",
        "        # Server aggregates and updates its model\n",
        "        main_server.aggregate_and_update()\n",
        "        \n",
        "        # Clients compute gradients and apply Gaussian noise\n",
        "        for client in clients:\n",
        "            if client.client_id in client_activation_grads:\n",
        "                activation_grad = client_activation_grads[client.client_id]\n",
        "                noisy_wc_grad = client.local_backward_pass(activation_grad)\n",
        "                client_noisy_wc_grads.append(noisy_wc_grad)\n",
        "                \n",
        "                # Track Gaussian privacy cost\n",
        "                if current_sigma > 0:\n",
        "                    privacy_accountant.gaussian_step(noise_multiplier=current_sigma)\n",
        "            else:\n",
        "                print(f\"Warning: No activation gradient received for Client {client.client_id}\")\n",
        "        \n",
        "        # Fed server aggregates client updates\n",
        "        for noisy_grad in client_noisy_wc_grads:\n",
        "            fed_server.receive_client_update(noisy_grad)\n",
        "        \n",
        "        # Fed server updates global client model\n",
        "        fed_server.aggregate_updates(validation_loader, main_server)\n",
        "        \n",
        "        # Record metrics\n",
        "        if (round_num + 1) % log_interval == 0 or round_num == num_rounds - 1:\n",
        "            # Evaluate the combined model\n",
        "            # First update the full model with client and server parameters\n",
        "            client_params = fed_server.get_client_model_params()\n",
        "            server_params = main_server.get_server_model().state_dict()\n",
        "            \n",
        "            # Set client part parameters\n",
        "            for i, layer in enumerate(full_model.client_layers):\n",
        "                if i < config['split_layer']:\n",
        "                    for name, param in layer.state_dict().items():\n",
        "                        full_param_name = f\"{i}.{name}\"\n",
        "                        if full_param_name in client_params:\n",
        "                            param.copy_(client_params[full_param_name])\n",
        "            \n",
        "            # Set server part parameters\n",
        "            server_part_idx = 0\n",
        "            for i, layer in enumerate(full_model.server_layers):\n",
        "                for name, param in layer.state_dict().items():\n",
        "                    full_param_name = f\"{i}.{name}\"\n",
        "                    if full_param_name in server_params:\n",
        "                        param.copy_(server_params[full_param_name])\n",
        "            \n",
        "            # Evaluate on test set\n",
        "            test_accuracy = evaluate_model(full_model, test_loader, device)\n",
        "            metrics['test_accuracy'].append(test_accuracy)\n",
        "            \n",
        "            # Get privacy cost\n",
        "            epsilon, delta = privacy_accountant.get_privacy_spent(delta=config['dp_noise']['delta'])\n",
        "            metrics['epsilon'].append(epsilon)\n",
        "            metrics['epsilon_laplace'].append(privacy_accountant.epsilon_laplace)\n",
        "            metrics['epsilon_gaussian'].append(privacy_accountant.epsilon_gaussian)\n",
        "            metrics['sigma'].append(current_sigma)\n",
        "            \n",
        "            print(f\"\\nRound {round_num + 1}:\")\n",
        "            print(f\"  Test Accuracy: {test_accuracy:.2f}%\")\n",
        "            print(f\"  Privacy Budget (ε, δ): ({epsilon:.4f}, {config['dp_noise']['delta']})\")\n",
        "            print(f\"  Current Noise Scale (σ): {current_sigma:.4f}\")\n",
        "        \n",
        "        # Record round time\n",
        "        round_end_time = time.time()\n",
        "        metrics['round_times'].append(round_end_time - round_start_time)\n",
        "    \n",
        "    # Final metrics for results dictionary\n",
        "    if results_dict is not None:\n",
        "        results_dict[config['id']] = {\n",
        "            'config': config,\n",
        "            'final_test_acc': metrics['test_accuracy'][-1],\n",
        "            'final_epsilon': metrics['epsilon'][-1],\n",
        "            'final_epsilon_laplace': metrics['epsilon_laplace'][-1],\n",
        "            'final_epsilon_gaussian': metrics['epsilon_gaussian'][-1],\n",
        "            'final_sigma': metrics['sigma'][-1],\n",
        "            'test_accuracy_history': metrics['test_accuracy'],\n",
        "            'epsilon_history': metrics['epsilon'],\n",
        "            'sigma_history': metrics['sigma'],\n",
        "            'total_training_time': sum(metrics['round_times'])\n",
        "        }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluates the model on the test dataset.\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run experiments and save results\n",
        "def run_experiments(configs, results_csv_path='results.csv', subset=None):\n",
        "    \"\"\"\n",
        "    Run experiments based on configurations and save results.\n",
        "    \n",
        "    Args:\n",
        "        configs: List of configuration dictionaries\n",
        "        results_csv_path: Path to save CSV results\n",
        "        subset: Optional integer to run only a subset of configurations\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary of results\n",
        "    \"\"\"\n",
        "    # Subset configurations if requested\n",
        "    if subset is not None and subset > 0:\n",
        "        configs = configs[:subset]\n",
        "    \n",
        "    # Dictionary to store results\n",
        "    results = {}\n",
        "    \n",
        "    # Run each experiment\n",
        "    for i, config in enumerate(configs):\n",
        "        print(f\"\\n\\n{'='*80}\")\n",
        "        print(f\"Experiment {i+1}/{len(configs)}: {config['id']}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Run the experiment\n",
        "        try:\n",
        "            metrics = train_sfl(config, results)\n",
        "            \n",
        "            # Add results to dictionary\n",
        "            results[config['id']] = {\n",
        "                'config': config,\n",
        "                'metrics': metrics\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error in experiment {config['id']}: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    \n",
        "    # Convert results to DataFrame\n",
        "    results_df = pd.DataFrame([\n",
        "        {\n",
        "            'experiment_id': exp_id,\n",
        "            'dataset': results[exp_id]['config']['dataset'],\n",
        "            'model': results[exp_id]['config']['model'],\n",
        "            'dp_mode': results[exp_id]['config']['dp_noise']['mode'],\n",
        "            'distribution': results[exp_id]['config']['partition_method'],\n",
        "            'alpha': results[exp_id]['config'].get('dirichlet_alpha', 'N/A'),\n",
        "            'num_clients': results[exp_id]['config']['num_clients'],\n",
        "            'final_test_acc': results[exp_id]['final_test_acc'],\n",
        "            'final_epsilon': results[exp_id]['final_epsilon'],\n",
        "            'final_epsilon_laplace': results[exp_id]['final_epsilon_laplace'],\n",
        "            'final_epsilon_gaussian': results[exp_id]['final_epsilon_gaussian'],\n",
        "            'final_sigma': results[exp_id]['final_sigma'],\n",
        "            'total_training_time': results[exp_id]['total_training_time']\n",
        "        }\n",
        "        for exp_id in results if 'final_test_acc' in results[exp_id]\n",
        "    ])\n",
        "    \n",
        "    # Save results to CSV\n",
        "    results_df.to_csv(results_csv_path, index=False)\n",
        "    print(f\"Results saved to {results_csv_path}\")\n",
        "    \n",
        "    return results, results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization functions\n",
        "def plot_accuracy_vs_epsilon(results_df, dataset=None, distribution=None, num_clients=None):\n",
        "    \"\"\"\n",
        "    Plot accuracy vs epsilon for different DP modes.\n",
        "    \n",
        "    Args:\n",
        "        results_df: DataFrame of results\n",
        "        dataset: Optional filter for dataset\n",
        "        distribution: Optional filter for distribution ('iid' or 'dirichlet')\n",
        "        num_clients: Optional filter for number of clients\n",
        "    \"\"\"\n",
        "    # Filter results\n",
        "    filtered_df = results_df.copy()\n",
        "    if dataset is not None:\n",
        "        filtered_df = filtered_df[filtered_df['dataset'] == dataset]\n",
        "    if distribution is not None:\n",
        "        filtered_df = filtered_df[filtered_df['distribution'] == distribution]\n",
        "    if num_clients is not None:\n",
        "        filtered_df = filtered_df[filtered_df['num_clients'] == num_clients]\n",
        "    \n",
        "    # Group by DP mode\n",
        "    grouped = filtered_df.groupby('dp_mode')\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    for name, group in grouped:\n",
        "        plt.scatter(\n",
        "            group['final_epsilon'], \n",
        "            group['final_test_acc'], \n",
        "            label=name, \n",
        "            alpha=0.7,\n",
        "            s=50\n",
        "        )\n",
        "    \n",
        "    plt.xlabel('Privacy Budget (ε)')\n",
        "    plt.ylabel('Test Accuracy')\n",
        "    plt.title('Accuracy vs Privacy Budget by DP Mode')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracy_vs_distribution(results_df, dataset=None, dp_mode=None):\n",
        "    \"\"\"\n",
        "    Plot accuracy for different data distributions.\n",
        "    \n",
        "    Args:\n",
        "        results_df: DataFrame of results\n",
        "        dataset: Optional filter for dataset\n",
        "        dp_mode: Optional filter for DP mode\n",
        "    \"\"\"\n",
        "    # Filter results\n",
        "    filtered_df = results_df.copy()\n",
        "    if dataset is not None:\n",
        "        filtered_df = filtered_df[filtered_df['dataset'] == dataset]\n",
        "    if dp_mode is not None:\n",
        "        filtered_df = filtered_df[filtered_df['dp_mode'] == dp_mode]\n",
        "    \n",
        "    # Create distribution category for grouping\n",
        "    def get_distribution_category(row):\n",
        "        if row['distribution'] == 'iid':\n",
        "            return 'IID'\n",
        "        else:  # dirichlet\n",
        "            alpha = row['alpha']\n",
        "            if alpha <= 0.1:\n",
        "                return 'Highly Non-IID (α=0.1)'\n",
        "            elif alpha <= 0.5:\n",
        "                return 'Moderately Non-IID (α=0.5)'\n",
        "            elif alpha <= 1.0:\n",
        "                return 'Slightly Non-IID (α=1.0)'\n",
        "            else:\n",
        "                return 'Nearly IID (α=10.0)'\n",
        "    \n",
        "    filtered_df['distribution_category'] = filtered_df.apply(get_distribution_category, axis=1)\n",
        "    \n",
        "    # Group by distribution category\n",
        "    grouped = filtered_df.groupby(['distribution_category', 'num_clients'])\n",
        "    \n",
        "    # Calculate mean accuracy for each group\n",
        "    mean_acc = grouped['final_test_acc'].mean().reset_index()\n",
        "    pivot_df = mean_acc.pivot(index='distribution_category', columns='num_clients', values='final_test_acc')\n",
        "    \n",
        "    # Sort categories\n",
        "    category_order = [\n",
        "        'IID', \n",
        "        'Nearly IID (α=10.0)', \n",
        "        'Slightly Non-IID (α=1.0)', \n",
        "        'Moderately Non-IID (α=0.5)', \n",
        "        'Highly Non-IID (α=0.1)'\n",
        "    ]\n",
        "    pivot_df = pivot_df.reindex(category_order)\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    pivot_df.plot(kind='bar', ax=plt.gca())\n",
        "    \n",
        "    plt.xlabel('Data Distribution')\n",
        "    plt.ylabel('Average Test Accuracy')\n",
        "    plt.title('Impact of Data Distribution on Accuracy')\n",
        "    plt.legend(title='Number of Clients')\n",
        "    plt.grid(True, axis='y', alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_privacy_decay(results, exp_id):\n",
        "    \"\"\"\n",
        "    Plot privacy parameter decay over training rounds.\n",
        "    \n",
        "    Args:\n",
        "        results: Dictionary of results\n",
        "        exp_id: Experiment ID to plot\n",
        "    \"\"\"\n",
        "    if exp_id not in results or 'metrics' not in results[exp_id]:\n",
        "        print(f\"No results found for experiment {exp_id}\")\n",
        "        return\n",
        "    \n",
        "    metrics = results[exp_id]['metrics']\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # Plot in a 2x1 grid\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(metrics['epsilon'], label='Total ε', marker='o')\n",
        "    plt.plot(metrics['epsilon_laplace'], label='Laplace ε', marker='s')\n",
        "    plt.plot(metrics['epsilon_gaussian'], label='Gaussian ε', marker='^')\n",
        "    plt.xlabel('Logging Step')\n",
        "    plt.ylabel('Privacy Budget (ε)')\n",
        "    plt.title('Privacy Budget Evolution')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(metrics['sigma'], label='Noise Scale (σ)', marker='o')\n",
        "    plt.plot(metrics['test_accuracy'], label='Test Accuracy', marker='s')\n",
        "    plt.xlabel('Logging Step')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Noise Scale and Accuracy Evolution')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run a subset of experiments to demonstrate functionality\n",
        "# Note: Running all experiments may take several hours or days\n",
        "# You can adjust the subset size as needed\n",
        "\n",
        "# Reduce number of rounds for faster execution\n",
        "for config in experiment_configs:\n",
        "    config['num_rounds'] = 10  # Reduced from 50 for demonstration\n",
        "    config['log_interval'] = 2  # Log more frequently\n",
        "\n",
        "# Run a small subset of experiments for demonstration\n",
        "subset_size = 4  # Adjust this based on available computation time\n",
        "results, results_df = run_experiments(experiment_configs, results_csv_path='results.csv', subset=subset_size)\n",
        "\n",
        "# Display results summary\n",
        "print(\"\\nResults Summary:\")\n",
        "print(results_df[['experiment_id', 'dataset', 'dp_mode', 'distribution', 'alpha', 'num_clients', 'final_test_acc', 'final_epsilon']])\n",
        "\n",
        "# Plot results\n",
        "if len(results_df) > 0:\n",
        "    # Plot accuracy vs privacy budget\n",
        "    plot_accuracy_vs_epsilon(results_df)\n",
        "    \n",
        "    # Plot first experiment's privacy decay\n",
        "    first_exp_id = list(results.keys())[0]\n",
        "    plot_privacy_decay(results, first_exp_id)\n",
        "    \n",
        "    # If we have enough data with different distributions\n",
        "    if len(results_df['distribution'].unique()) > 1:\n",
        "        plot_accuracy_vs_distribution(results_df)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Results Analysis and Conclusions\n",
        "\n",
        "After running the experiments, we can draw conclusions about:\n",
        "\n",
        "1. **Privacy-Utility Tradeoff**: How does increasing privacy protection (lower ε) affect model accuracy?\n",
        "2. **Vanilla vs Adaptive DP**: Does adaptive noise scheduling provide better privacy-utility tradeoff?\n",
        "3. **Data Distribution Impact**: How does non-IID data distribution affect training with DP?\n",
        "4. **Scalability**: How does the number of clients affect privacy and utility?\n",
        "\n",
        "The CSV file `results.csv` contains all the experiment results for further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional analysis functions\n",
        "\n",
        "def compare_dp_modes(results_df, metric='final_test_acc'):\n",
        "    \"\"\"Compare different DP modes across all experiments.\"\"\"\n",
        "    grouped = results_df.groupby('dp_mode')[metric].agg(['mean', 'std', 'min', 'max'])\n",
        "    \n",
        "    print(f\"Comparison of DP modes by {metric}:\")\n",
        "    print(grouped)\n",
        "    \n",
        "    # Create box plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    results_df.boxplot(column=metric, by='dp_mode', grid=False)\n",
        "    plt.title(f'Distribution of {metric} by DP Mode')\n",
        "    plt.suptitle('')  # Remove default title\n",
        "    plt.ylabel(metric)\n",
        "    plt.show()\n",
        "\n",
        "def calculate_privacy_utility_ratio(results_df):\n",
        "    \"\"\"Calculate and compare privacy-utility ratio (accuracy/epsilon).\"\"\"\n",
        "    # Higher ratio means better utility per privacy cost\n",
        "    results_df['privacy_utility_ratio'] = results_df['final_test_acc'] / results_df['final_epsilon']\n",
        "    \n",
        "    grouped = results_df.groupby('dp_mode')['privacy_utility_ratio'].agg(['mean', 'std', 'min', 'max'])\n",
        "    \n",
        "    print(\"Privacy-Utility Ratio (Accuracy/Epsilon) - higher is better:\")\n",
        "    print(grouped)\n",
        "    \n",
        "    # Create box plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    results_df.boxplot(column='privacy_utility_ratio', by='dp_mode', grid=False)\n",
        "    plt.title('Privacy-Utility Ratio by DP Mode')\n",
        "    plt.suptitle('')  # Remove default title\n",
        "    plt.ylabel('Accuracy/Epsilon Ratio')\n",
        "    plt.show()\n",
        "    \n",
        "    return results_df\n",
        "\n",
        "# Execute these analyses if we have results\n",
        "if 'results_df' in locals() and len(results_df) > 0:\n",
        "    compare_dp_modes(results_df, 'final_test_acc')\n",
        "    compare_dp_modes(results_df, 'final_epsilon')\n",
        "    results_df = calculate_privacy_utility_ratio(results_df)\n",
        "else:\n",
        "    print(\"No results available yet. Run the experiments first.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
