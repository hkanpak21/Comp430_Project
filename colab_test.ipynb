{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secure Split-FL Testing on Google Colab\n",
    "\n",
    "This notebook sets up and runs the Secure Split-FL tests using GPU acceleration on Google Colab.\n",
    "\n",
    "## Setup Instructions\n",
    "1. Upload this notebook to Google Colab\n",
    "2. Make sure you have selected GPU runtime (Runtime -> Change runtime type -> GPU)\n",
    "3. Run each cell in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/YOUR_USERNAME/Comp430_Project.git\n",
    "%cd Comp430_Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install -r requirements.txt pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create necessary directories\n",
    "!mkdir -p experiments/out\n",
    "!mkdir -p experiments/tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%writefile experiments/tests/utils.py\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import subprocess\n",
    "import uuid\n",
    "import yaml\n",
    "import datetime\n",
    "\n",
    "ROOT = pathlib.Path(__file__).resolve().parents[1]\n",
    "OUT  = ROOT / \"out\"\n",
    "\n",
    "def run_experiment(cfg_path: str):\n",
    "    \"\"\"Launch one SFL run, return metrics dict.\"\"\"\n",
    "    run_id = f\"{pathlib.Path(cfg_path).stem}__{uuid.uuid4().hex[:6]}\"\n",
    "    run_dir = OUT / run_id\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1. spawn training as subprocess\n",
    "    cmd = [\n",
    "        \"python\", \"experiments/train_secure_sfl.py\",\n",
    "        \"--config\", cfg_path,\n",
    "        \"--run_id\", run_id                  \n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "\n",
    "    # 2. collect metrics saved by the runner\n",
    "    metrics_file = run_dir / \"metrics.json\"\n",
    "    with open(metrics_file) as f:\n",
    "        metrics = json.load(f)\n",
    "\n",
    "    # 3. additionally log stdout / stderr\n",
    "    (run_dir / \"stdout.txt\").write_text(result.stdout)\n",
    "    (run_dir / \"stderr.txt\").write_text(result.stderr)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%writefile experiments/tests/test_pipeline.py\n",
    "from .utils import run_experiment\n",
    "\n",
    "def test_forward_backward():\n",
    "    \"\"\"Tests that the pipeline works by running a basic experiment.\"\"\"\n",
    "    m = run_experiment(\"configs/default.yaml\")\n",
    "    assert 0.0 < m[\"final_test_acc\"] < 1.0,  \"accuracy not logged\"\n",
    "    assert m[\"rounds\"] >= 1,                 \"training never started\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%writefile experiments/tests/test_dp_budget.py\n",
    "from .utils import run_experiment\n",
    "\n",
    "MAX_EPS = 8.0\n",
    "def test_fixed_dp():\n",
    "    \"\"\"Tests that the fixed DP budget does not exceed the maximum epsilon.\"\"\"\n",
    "    m = run_experiment(\"configs/fixed_dp.yaml\")\n",
    "    assert m[\"epsilon\"] <= MAX_EPS, \"ε budget exceeded\"\n",
    "\n",
    "def test_adaptive_sigma():\n",
    "    \"\"\"Tests that the adaptive noise mechanism reduces noise over time.\"\"\"\n",
    "    m = run_experiment(\"configs/adaptive_dp.yaml\")\n",
    "    assert m[\"sigma\"] < m[\"sigma_init\"], \"σ did not decay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%writefile experiments/tests/test_fedavg_equiv.py\n",
    "import torch\n",
    "from .utils import run_experiment\n",
    "\n",
    "THRESH = 1e-3\n",
    "def test_single_client_equivalence():\n",
    "    \"\"\"Tests that 1-client SFL is equivalent to centralized training.\"\"\"\n",
    "    m_split = run_experiment(\"configs/one_client.yaml\")\n",
    "    m_central = run_experiment(\"configs/central.yaml\")\n",
    "    diff = abs(m_split[\"final_test_acc\"] - m_central[\"final_test_acc\"])\n",
    "    assert diff < THRESH, f\"Split = {m_split}, Central = {m_central}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%writefile experiments/tests/test_utility.py\n",
    "import yaml\n",
    "from .utils import run_experiment\n",
    "\n",
    "def test_minimum_accuracy():\n",
    "    \"\"\"Tests that the model achieves the minimum accuracy specified in the config file.\"\"\"\n",
    "    config_path = \"configs/dnn_default.yaml\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    min_acc = config.get('min_acc', 0.7)\n",
    "    \n",
    "    m = run_experiment(config_path)\n",
    "    assert m[\"final_test_acc\"] >= min_acc, f\"Model accuracy {m['final_test_acc']} below minimum threshold {min_acc}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%writefile experiments/tests/test_known_config.py\n",
    "from .utils import run_experiment\n",
    "\n",
    "def test_known_working_config():\n",
    "    \"\"\"Tests the known working configuration (cnn_adaptive_dp.yaml).\"\"\"\n",
    "    m = run_experiment(\"configs/cnn_adaptive_dp.yaml\")\n",
    "    assert m[\"final_test_acc\"] > 0.70, \"Expected accuracy of at least 70% with this config\"\n",
    "    assert m[\"sigma_init\"] == 0.0 or m[\"sigma\"] == 0.0, \"DP noise should be disabled in this test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%writefile experiments/tests/__init__.py\n",
    "# Empty init file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Individual Tests\n",
    "You can run specific test files to check different aspects of the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run basic pipeline test\n",
    "!pytest -v experiments/tests/test_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run DP budget tests\n",
    "!pytest -v experiments/tests/test_dp_budget.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run federated averaging equivalence test\n",
    "!pytest -v experiments/tests/test_fedavg_equiv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run utility test\n",
    "!pytest -v experiments/tests/test_utility.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run known configuration test\n",
    "!pytest -v experiments/tests/test_known_config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Tests\n",
    "Run all tests at once to verify the complete system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run all tests\n",
    "!pytest -v experiments/tests/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Secure Split-FL Testing",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
