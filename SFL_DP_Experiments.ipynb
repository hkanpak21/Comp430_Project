{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Secure Split Federated Learning (SFL) with DP - Experiment Suite\n",
        "\n",
        "This notebook provides an automated experiment environment for the SFL-DP framework. It will:\n",
        "1. Clone the repository\n",
        "2. Install dependencies\n",
        "3. Run experiments across various datasets, client configurations, SFL split strategies, and DP mechanisms\n",
        "4. Visualize and analyze the results\n",
        "\n",
        "**Note:** Make sure to run this notebook with GPU enabled (Runtime → Change runtime type → Hardware accelerator → GPU)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup Environment\n",
        "\n",
        "First, we'll check if we have GPU available, clone the repository, and install dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "# Clone the repository specifically from the privacy_accountant branch\n",
        "!git clone -b privacy_accountant https://github.com/YOUR_USERNAME/Comp430_Project.git\n",
        "%cd Comp430_Project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add repo path to Python path for imports\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# Set project root\n",
        "project_root = os.getcwd()\n",
        "sys.path.insert(0, project_root)\n",
        "print(f\"Project root: {project_root}\")\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path(project_root) / \"experiments\" / \"out\" / \"colab_runs\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Output directory: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Experiment Configuration\n",
        "\n",
        "Let's define our experiment suite. We'll run a combination of experiments with different:\n",
        "- Datasets (MNIST, BCW)\n",
        "- Client counts (3, 5, 10, 20)\n",
        "- DP mechanisms (vanilla, adaptive)\n",
        "- Split strategies (various cut layers, FL, centralized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List available configuration files\n",
        "config_files = list(Path(project_root).glob(\"configs/*.yaml\"))\n",
        "print(f\"Found {len(config_files)} configuration files:\")\n",
        "for config_file in sorted(config_files):\n",
        "    print(f\"- {config_file.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the baseline comparison experiments with different noise settings\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Define the specific configs for our baseline comparison\n",
        "baseline_configs = [\n",
        "    \"mnist_clients5_no_noise.yaml\",\n",
        "    \"mnist_clients5_fixed_noise.yaml\",\n",
        "    \"mnist_clients5_adaptive_dp_09.yaml\",\n",
        "    \"mnist_clients5_adaptive_dp_08.yaml\",\n",
        "    \"mnist_clients5_adaptive_dp_05.yaml\"\n",
        "]\n",
        "\n",
        "# Initialize results storage\n",
        "baseline_results = []\n",
        "\n",
        "# Create progress bar\n",
        "pbar = tqdm(total=len(baseline_configs), desc=\"Baseline Experiments\")\n",
        "\n",
        "# Run each config file\n",
        "for config_file in baseline_configs:\n",
        "    config_path = f\"configs/{config_file}\"\n",
        "    run_id = config_file.replace('.yaml', '')\n",
        "    \n",
        "    # Generate a description from the filename\n",
        "    if \"no_noise\" in run_id:\n",
        "        desc = \"No DP Noise\"\n",
        "    elif \"fixed_noise\" in run_id:\n",
        "        desc = \"Fixed DP Noise\"\n",
        "    elif \"adaptive_dp_09\" in run_id:\n",
        "        desc = \"Adaptive DP (0.9 decay)\"\n",
        "    elif \"adaptive_dp_08\" in run_id:\n",
        "        desc = \"Adaptive DP (0.8 decay)\"\n",
        "    elif \"adaptive_dp_05\" in run_id:\n",
        "        desc = \"Adaptive DP (0.5 decay)\"\n",
        "    else:\n",
        "        desc = run_id\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Running baseline experiment: {desc}\")\n",
        "    print(f\"Config file: {config_file}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Measure execution time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Run the experiment\n",
        "        cmd = f\"python experiments/train_secure_sfl.py --config {config_path} --run_id {run_id}\"\n",
        "        print(f\"Command: {cmd}\")\n",
        "        !{cmd}\n",
        "        \n",
        "        end_time = time.time()\n",
        "        total_runtime = end_time - start_time\n",
        "        \n",
        "        # Record result\n",
        "        result = {\n",
        "            \"config\": config_file,\n",
        "            \"run_id\": run_id,\n",
        "            \"description\": desc,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "            \"total_runtime\": total_runtime,\n",
        "            \"metrics_path\": f\"experiments/out/{run_id}/metrics.json\",\n",
        "            \"status\": \"completed\"\n",
        "        }\n",
        "        \n",
        "        print(f\"\\nExperiment completed in {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Handle errors\n",
        "        print(f\"\\n{'!'*80}\")\n",
        "        print(f\"Error running experiment with {config_file}: {str(e)}\")\n",
        "        print(f\"{'!'*80}\")\n",
        "        \n",
        "        end_time = time.time()\n",
        "        total_runtime = end_time - start_time\n",
        "        \n",
        "        # Record failure\n",
        "        result = {\n",
        "            \"config\": config_file,\n",
        "            \"run_id\": run_id,\n",
        "            \"description\": desc,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "            \"total_runtime\": total_runtime,\n",
        "            \"error\": str(e),\n",
        "            \"status\": \"failed\"\n",
        "        }\n",
        "    \n",
        "    # Add to results\n",
        "    baseline_results.append(result)\n",
        "    \n",
        "    # Update progress\n",
        "    pbar.update(1)\n",
        "\n",
        "# Close progress bar\n",
        "pbar.close()\n",
        "\n",
        "# Save experiment summary\n",
        "baseline_summary_file = output_dir / \"baseline_experiment_summary.json\"\n",
        "with open(baseline_summary_file, 'w') as f:\n",
        "    json.dump(baseline_results, f, indent=2)\n",
        "    \n",
        "print(f\"\\nAll {len(baseline_configs)} baseline experiments completed\")\n",
        "print(f\"Summary saved to {baseline_summary_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run client comparison experiments (3, 5, 10, 20, 50 clients) with both Adaptive and Vanilla DP\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Define the specific configs for client comparison\n",
        "client_configs = [\n",
        "    # Adaptive DP configs\n",
        "    \"mnist_clients3_adaptive_dp.yaml\",\n",
        "    \"mnist_clients5_adaptive_dp.yaml\",\n",
        "    \"mnist_clients10_adaptive_dp.yaml\",\n",
        "    \"mnist_clients20_adaptive_dp.yaml\",\n",
        "    \"mnist_clients50_adaptive_dp.yaml\",\n",
        "    # Vanilla DP configs\n",
        "    \"mnist_clients3_vanilla_dp.yaml\",\n",
        "    \"mnist_clients5_vanilla_dp.yaml\",\n",
        "    \"mnist_clients10_vanilla_dp.yaml\",\n",
        "    \"mnist_clients20_vanilla_dp.yaml\",\n",
        "    \"mnist_clients50_vanilla_dp.yaml\"\n",
        "]\n",
        "\n",
        "# Initialize results storage\n",
        "client_results = []\n",
        "all_accuracy_data = []\n",
        "\n",
        "# Create progress bar\n",
        "pbar = tqdm(total=len(client_configs), desc=\"Client Comparison Experiments\")\n",
        "\n",
        "# Run each config file\n",
        "for config_file in client_configs:\n",
        "    config_path = f\"configs/{config_file}\"\n",
        "    run_id = config_file.replace('.yaml', '')\n",
        "    \n",
        "    # Skip if config file doesn't exist\n",
        "    if not os.path.exists(config_path):\n",
        "        print(f\"Config file {config_path} does not exist. Skipping.\")\n",
        "        pbar.update(1)\n",
        "        continue\n",
        "    \n",
        "    # Generate a description from the filename\n",
        "    client_count = re.search(r'clients(\\d+)', config_file).group(1)\n",
        "    dp_type = \"Adaptive\" if \"adaptive\" in config_file else \"Vanilla\"\n",
        "    desc = f\"{client_count} Clients - {dp_type} DP\"\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Running client comparison experiment: {desc}\")\n",
        "    print(f\"Config file: {config_file}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Measure execution time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Run the experiment and capture output\n",
        "        cmd = f\"python experiments/train_secure_sfl.py --config {config_path} --run_id {run_id}\"\n",
        "        print(f\"Command: {cmd}\")\n",
        "        output = !{cmd}\n",
        "        # Convert the output to a string\n",
        "        output_text = \"\\n\".join(output)\n",
        "        \n",
        "        # Save the output to a file for parsing\n",
        "        output_file = f\"result_{run_id}.txt\"\n",
        "        with open(output_file, \"w\") as f:\n",
        "            f.write(output_text)\n",
        "        \n",
        "        # Parse the output file to extract accuracy data\n",
        "        with open(output_file, 'r') as f:\n",
        "            content = f.read()\n",
        "        \n",
        "        # Extract rounds and validation accuracies\n",
        "        round_pattern = r\"--- Round (\\d+)/50 ---.*?Validation Metrics:\\nComplete Model - Loss: [\\d.]+ \\| Accuracy: ([\\d.]+)%\"\n",
        "        val_matches = re.findall(round_pattern, content, re.DOTALL)\n",
        "        \n",
        "        # Extract test accuracies\n",
        "        test_acc_pattern = r\"Round (\\d+): Test Accuracy: ([\\d.]+)%\"\n",
        "        test_matches = re.findall(test_acc_pattern, content)\n",
        "        \n",
        "        # Store validation accuracies\n",
        "        for round_num, accuracy in val_matches:\n",
        "            all_accuracy_data.append({\n",
        "                'client_count': int(client_count),\n",
        "                'dp_type': dp_type,\n",
        "                'round': int(round_num),\n",
        "                'accuracy_type': 'Validation',\n",
        "                'accuracy': float(accuracy)\n",
        "            })\n",
        "        \n",
        "        # Store test accuracies\n",
        "        for round_num, accuracy in test_matches:\n",
        "            all_accuracy_data.append({\n",
        "                'client_count': int(client_count),\n",
        "                'dp_type': dp_type,\n",
        "                'round': int(round_num),\n",
        "                'accuracy_type': 'Test',\n",
        "                'accuracy': float(accuracy)\n",
        "            })\n",
        "        \n",
        "        end_time = time.time()\n",
        "        total_runtime = end_time - start_time\n",
        "        \n",
        "        # Record result\n",
        "        result = {\n",
        "            \"config\": config_file,\n",
        "            \"run_id\": run_id,\n",
        "            \"description\": desc,\n",
        "            \"client_count\": int(client_count),\n",
        "            \"dp_type\": dp_type,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "            \"total_runtime\": total_runtime,\n",
        "            \"metrics_path\": f\"experiments/out/{run_id}/metrics.json\",\n",
        "            \"status\": \"completed\"\n",
        "        }\n",
        "        \n",
        "        print(f\"\\nExperiment completed in {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Handle errors\n",
        "        print(f\"\\n{'!'*80}\")\n",
        "        print(f\"Error running experiment with {config_file}: {str(e)}\")\n",
        "        print(f\"{'!'*80}\")\n",
        "        \n",
        "        end_time = time.time()\n",
        "        total_runtime = end_time - start_time\n",
        "        \n",
        "        # Record failure\n",
        "        result = {\n",
        "            \"config\": config_file,\n",
        "            \"run_id\": run_id,\n",
        "            \"description\": desc,\n",
        "            \"client_count\": int(client_count),\n",
        "            \"dp_type\": dp_type,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "            \"total_runtime\": total_runtime,\n",
        "            \"error\": str(e),\n",
        "            \"status\": \"failed\"\n",
        "        }\n",
        "    \n",
        "    # Add to results\n",
        "    client_results.append(result)\n",
        "    \n",
        "    # Update progress\n",
        "    pbar.update(1)\n",
        "    \n",
        "    # Save intermediate results after each experiment\n",
        "    accuracy_df = pd.DataFrame(all_accuracy_data)\n",
        "    accuracy_df.to_csv('client_comparison_accuracy.csv', index=False)\n",
        "\n",
        "# Close progress bar\n",
        "pbar.close()\n",
        "\n",
        "# Save experiment summary\n",
        "client_summary_file = output_dir / \"client_comparison_summary.json\"\n",
        "with open(client_summary_file, 'w') as f:\n",
        "    json.dump(client_results, f, indent=2)\n",
        "\n",
        "# Create and save final accuracy DataFrame\n",
        "accuracy_df = pd.DataFrame(all_accuracy_data)\n",
        "accuracy_df.to_csv('client_comparison_accuracy.csv', index=False)\n",
        "    \n",
        "print(f\"\\nAll {len(client_configs)} client comparison experiments completed\")\n",
        "print(f\"Summary saved to {client_summary_file}\")\n",
        "print(f\"Accuracy data saved to client_comparison_accuracy.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize client comparison results\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Check if the accuracy data file exists\n",
        "try:\n",
        "    accuracy_df = pd.read_csv('client_comparison_accuracy.csv')\n",
        "    has_data = True\n",
        "except FileNotFoundError:\n",
        "    print(\"No accuracy data file found. Run the client comparison experiments first.\")\n",
        "    has_data = False\n",
        "\n",
        "if has_data:\n",
        "    # Set the style for plots\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')  # Updated style name\n",
        "    plt.rcParams['axes.facecolor'] = '#f8f8f8'  # Light gray background\n",
        "    plt.rcParams['grid.color'] = '#cccccc'      # Lighter grid lines\n",
        "    \n",
        "    # Define colors for different client numbers\n",
        "    colors = {\n",
        "        3: '#8B0000',    # Dark Red\n",
        "        5: '#FF0000',    # Red\n",
        "        10: '#FF4500',   # Orange Red\n",
        "        20: '#FF8C00',   # Dark Orange\n",
        "        50: '#B22222',   # Firebrick\n",
        "    }\n",
        "    \n",
        "    # Define line styles for different DP methods\n",
        "    line_styles = {\n",
        "        'Adaptive': '-',      # Solid\n",
        "        'Vanilla': '--'       # Dashed\n",
        "    }\n",
        "    \n",
        "    # Define marker styles\n",
        "    markers = {\n",
        "        3: 'o',    # Circle\n",
        "        5: 's',    # Square\n",
        "        10: '^',   # Triangle\n",
        "        20: 'D',   # Diamond\n",
        "        50: 'X'    # X\n",
        "    }\n",
        "    \n",
        "    # Filter for test accuracy data\n",
        "    test_df = accuracy_df[accuracy_df['accuracy_type'] == 'Test']\n",
        "    \n",
        "    # Create test accuracy plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    for client_count in sorted(test_df['client_count'].unique()):\n",
        "        for dp_type in sorted(test_df['dp_type'].unique()):\n",
        "            data = test_df[(test_df['client_count'] == client_count) & (test_df['dp_type'] == dp_type)]\n",
        "            if not data.empty:\n",
        "                label = f\"{client_count} Clients - {dp_type}\"\n",
        "                plt.plot(data['round'], data['accuracy'], \n",
        "                        color=colors.get(client_count, '#000000'), \n",
        "                        linestyle=line_styles.get(dp_type, '-'),\n",
        "                        linewidth=2.5, \n",
        "                        marker=markers.get(client_count, 'o'), \n",
        "                        markersize=8,\n",
        "                        label=label)\n",
        "    \n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.title('Test Accuracy Comparison by Client Count', fontsize=18, fontweight='bold')\n",
        "    plt.xlabel('Training Round', fontsize=16)\n",
        "    plt.ylabel('Test Accuracy (%)', fontsize=16)\n",
        "    plt.legend(fontsize=12, frameon=True, facecolor='white', edgecolor='#CCCCCC', loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plt.savefig(\"client_comparison_test_accuracy.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Filter for validation accuracy data\n",
        "    val_df = accuracy_df[accuracy_df['accuracy_type'] == 'Validation']\n",
        "    \n",
        "    # Create validation accuracy plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    for client_count in sorted(val_df['client_count'].unique()):\n",
        "        for dp_type in sorted(val_df['dp_type'].unique()):\n",
        "            data = val_df[(val_df['client_count'] == client_count) & (val_df['dp_type'] == dp_type)]\n",
        "            if not data.empty:\n",
        "                label = f\"{client_count} Clients - {dp_type}\"\n",
        "                plt.plot(data['round'], data['accuracy'], \n",
        "                        color=colors.get(client_count, '#000000'), \n",
        "                        linestyle=line_styles.get(dp_type, '-'),\n",
        "                        linewidth=2.5, \n",
        "                        marker=markers.get(client_count, 'o'), \n",
        "                        markersize=6, \n",
        "                        markevery=5,  # Show markers every 5 points to reduce clutter\n",
        "                        label=label)\n",
        "    \n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.title('Validation Accuracy Comparison by Client Count', fontsize=18, fontweight='bold')\n",
        "    plt.xlabel('Training Round', fontsize=16)\n",
        "    plt.ylabel('Validation Accuracy (%)', fontsize=16)\n",
        "    plt.legend(fontsize=12, frameon=True, facecolor='white', edgecolor='#CCCCCC', loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plt.savefig(\"client_comparison_validation_accuracy.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Create a summary table of final test accuracies\n",
        "    final_test_df = test_df.sort_values('round', ascending=False).drop_duplicates(['client_count', 'dp_type'])\n",
        "    final_test_pivot = final_test_df.pivot_table(\n",
        "        index='client_count', \n",
        "        columns='dp_type', \n",
        "        values='accuracy'\n",
        "    ).reset_index()\n",
        "    \n",
        "    print(\"Final Test Accuracies:\")\n",
        "    display(final_test_pivot)\n",
        "    \n",
        "    # Save the pivot table to CSV\n",
        "    final_test_pivot.to_csv('client_comparison_final_accuracy.csv')\n",
        "    print(\"Final accuracies saved to client_comparison_final_accuracy.csv\")\n",
        "else:\n",
        "    print(\"Please run the client comparison experiments first to generate data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run cut layer comparison experiments with 5 clients for both Adaptive and Vanilla DP\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Define the specific configs for cut layer comparison\n",
        "cut_layer_configs = [\n",
        "    # Adaptive DP configs with different cut layers\n",
        "    \"mnist_clients5_cut_layer2_adaptive_dp.yaml\",\n",
        "    \"mnist_clients5_cut_layer4_adaptive_dp.yaml\",\n",
        "    \"mnist_clients5_cut_layer6_adaptive_dp.yaml\",\n",
        "    \"mnist_clients5_cut_layer8_adaptive_dp.yaml\",\n",
        "    # Vanilla DP configs with different cut layers\n",
        "    \"mnist_clients5_cut_layer2_vanilla_dp.yaml\",\n",
        "    \"mnist_clients5_cut_layer4_vanilla_dp.yaml\",\n",
        "    \"mnist_clients5_cut_layer6_vanilla_dp.yaml\",\n",
        "    \"mnist_clients5_cut_layer8_vanilla_dp.yaml\"\n",
        "]\n",
        "\n",
        "# Initialize results storage\n",
        "cut_layer_results = []\n",
        "all_accuracy_data = []\n",
        "\n",
        "# Create progress bar\n",
        "pbar = tqdm(total=len(cut_layer_configs), desc=\"Cut Layer Comparison Experiments\")\n",
        "\n",
        "# Run each config file\n",
        "for config_file in cut_layer_configs:\n",
        "    config_path = f\"configs/{config_file}\"\n",
        "    run_id = config_file.replace('.yaml', '')\n",
        "    \n",
        "    # Skip if config file doesn't exist\n",
        "    if not os.path.exists(config_path):\n",
        "        print(f\"Config file {config_path} does not exist. Skipping.\")\n",
        "        pbar.update(1)\n",
        "        continue\n",
        "    \n",
        "    # Generate a description from the filename\n",
        "    cut_layer = re.search(r'cut_layer(\\d+)', config_file).group(1)\n",
        "    dp_type = \"Adaptive\" if \"adaptive\" in config_file else \"Vanilla\"\n",
        "    desc = f\"Cut Layer {cut_layer} - {dp_type} DP\"\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Running cut layer comparison experiment: {desc}\")\n",
        "    print(f\"Config file: {config_file}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Measure execution time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Run the experiment and capture output\n",
        "        cmd = f\"python experiments/train_secure_sfl.py --config {config_path} --run_id {run_id}\"\n",
        "        print(f\"Command: {cmd}\")\n",
        "        output = !{cmd}\n",
        "        # Convert the output to a string\n",
        "        output_text = \"\\n\".join(output)\n",
        "        \n",
        "        # Save the output to a file for parsing\n",
        "        output_file = f\"result_{run_id}.txt\"\n",
        "        with open(output_file, \"w\") as f:\n",
        "            f.write(output_text)\n",
        "        \n",
        "        # Parse the output file to extract accuracy data\n",
        "        with open(output_file, 'r') as f:\n",
        "            content = f.read()\n",
        "        \n",
        "        # Extract rounds and validation accuracies\n",
        "        round_pattern = r\"--- Round (\\d+)/50 ---.*?Validation Metrics:\\nComplete Model - Loss: [\\d.]+ \\| Accuracy: ([\\d.]+)%\"\n",
        "        val_matches = re.findall(round_pattern, content, re.DOTALL)\n",
        "        \n",
        "        # Extract test accuracies\n",
        "        test_acc_pattern = r\"Round (\\d+): Test Accuracy: ([\\d.]+)%\"\n",
        "        test_matches = re.findall(test_acc_pattern, content)\n",
        "        \n",
        "        # Extract privacy budget\n",
        "        privacy_budget = \"N/A\"\n",
        "        final_budget_match = re.search(r\"Round 50: Current Privacy Budget \\(ε, δ=1e-05\\): \\(([\\d.e+]+)\", content)\n",
        "        if final_budget_match:\n",
        "            privacy_budget = final_budget_match.group(1)\n",
        "        \n",
        "        # Store validation accuracies\n",
        "        for round_num, accuracy in val_matches:\n",
        "            all_accuracy_data.append({\n",
        "                'cut_layer': int(cut_layer),\n",
        "                'dp_type': dp_type,\n",
        "                'round': int(round_num),\n",
        "                'accuracy_type': 'Validation',\n",
        "                'accuracy': float(accuracy)\n",
        "            })\n",
        "        \n",
        "        # Store test accuracies\n",
        "        for round_num, accuracy in test_matches:\n",
        "            all_accuracy_data.append({\n",
        "                'cut_layer': int(cut_layer),\n",
        "                'dp_type': dp_type,\n",
        "                'round': int(round_num),\n",
        "                'accuracy_type': 'Test',\n",
        "                'accuracy': float(accuracy),\n",
        "                'privacy_budget': privacy_budget if round_num == \"50\" else \"N/A\"\n",
        "            })\n",
        "        \n",
        "        end_time = time.time()\n",
        "        total_runtime = end_time - start_time\n",
        "        \n",
        "        # Record result\n",
        "        result = {\n",
        "            \"config\": config_file,\n",
        "            \"run_id\": run_id,\n",
        "            \"description\": desc,\n",
        "            \"cut_layer\": int(cut_layer),\n",
        "            \"dp_type\": dp_type,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "            \"total_runtime\": total_runtime,\n",
        "            \"privacy_budget\": privacy_budget,\n",
        "            \"metrics_path\": f\"experiments/out/{run_id}/metrics.json\",\n",
        "            \"status\": \"completed\"\n",
        "        }\n",
        "        \n",
        "        print(f\"\\nExperiment completed in {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Handle errors\n",
        "        print(f\"\\n{'!'*80}\")\n",
        "        print(f\"Error running experiment with {config_file}: {str(e)}\")\n",
        "        print(f\"{'!'*80}\")\n",
        "        \n",
        "        end_time = time.time()\n",
        "        total_runtime = end_time - start_time\n",
        "        \n",
        "        # Record failure\n",
        "        result = {\n",
        "            \"config\": config_file,\n",
        "            \"run_id\": run_id,\n",
        "            \"description\": desc,\n",
        "            \"cut_layer\": int(cut_layer),\n",
        "            \"dp_type\": dp_type,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "            \"total_runtime\": total_runtime,\n",
        "            \"error\": str(e),\n",
        "            \"status\": \"failed\"\n",
        "        }\n",
        "    \n",
        "    # Add to results\n",
        "    cut_layer_results.append(result)\n",
        "    \n",
        "    # Update progress\n",
        "    pbar.update(1)\n",
        "    \n",
        "    # Save intermediate results after each experiment\n",
        "    accuracy_df = pd.DataFrame(all_accuracy_data)\n",
        "    accuracy_df.to_csv('cut_layer_comparison_accuracy.csv', index=False)\n",
        "\n",
        "# Close progress bar\n",
        "pbar.close()\n",
        "\n",
        "# Save experiment summary\n",
        "cut_layer_summary_file = output_dir / \"cut_layer_comparison_summary.json\"\n",
        "with open(cut_layer_summary_file, 'w') as f:\n",
        "    json.dump(cut_layer_results, f, indent=2)\n",
        "\n",
        "# Create and save final accuracy DataFrame\n",
        "accuracy_df = pd.DataFrame(all_accuracy_data)\n",
        "accuracy_df.to_csv('cut_layer_comparison_accuracy.csv', index=False)\n",
        "\n",
        "# Create a summary table of final test accuracies\n",
        "test_df = accuracy_df[accuracy_df['accuracy_type'] == 'Test']\n",
        "final_test_df = test_df.sort_values('round', ascending=False).drop_duplicates(['cut_layer', 'dp_type'])\n",
        "final_test_pivot = final_test_df.pivot_table(\n",
        "    index='cut_layer', \n",
        "    columns='dp_type', \n",
        "    values=['accuracy', 'privacy_budget']\n",
        ").reset_index()\n",
        "\n",
        "# Save the pivot table to CSV\n",
        "final_test_pivot.to_csv('cut_layer_final_accuracy.csv')\n",
        "\n",
        "print(f\"\\nAll {len(cut_layer_configs)} cut layer comparison experiments completed\")\n",
        "print(f\"Summary saved to {cut_layer_summary_file}\")\n",
        "print(f\"Accuracy data saved to cut_layer_comparison_accuracy.csv\")\n",
        "print(f\"Final accuracies saved to cut_layer_final_accuracy.csv\")\n",
        "\n",
        "# Download the CSV files\n",
        "try:\n",
        "    files.download('cut_layer_comparison_accuracy.csv')\n",
        "    files.download('cut_layer_final_accuracy.csv')\n",
        "    print(\"CSV files downloaded successfully\")\n",
        "except:\n",
        "    print(\"Note: CSV download is only available in Google Colab. If you're running this locally, the files are saved in your working directory.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cut layer comparison results\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Check if the accuracy data file exists\n",
        "try:\n",
        "    accuracy_df = pd.read_csv('cut_layer_comparison_accuracy.csv')\n",
        "    has_data = True\n",
        "except FileNotFoundError:\n",
        "    print(\"No accuracy data file found. Run the cut layer comparison experiments first.\")\n",
        "    has_data = False\n",
        "\n",
        "if has_data:\n",
        "    # Set the style for plots\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')  # Updated style name\n",
        "    plt.rcParams['axes.facecolor'] = '#f8f8f8'  # Light gray background\n",
        "    plt.rcParams['grid.color'] = '#cccccc'      # Lighter grid lines\n",
        "    \n",
        "    # Define colors for different cut layers\n",
        "    colors = {\n",
        "        2: '#8B0000',    # Dark Red\n",
        "        4: '#FF0000',    # Red\n",
        "        6: '#FF4500',    # Orange Red\n",
        "        8: '#FF8C00',    # Dark Orange\n",
        "    }\n",
        "    \n",
        "    # Define line styles for different DP methods\n",
        "    line_styles = {\n",
        "        'Adaptive': '-',      # Solid\n",
        "        'Vanilla': '--'       # Dashed\n",
        "    }\n",
        "    \n",
        "    # Define marker styles\n",
        "    markers = {\n",
        "        2: 'o',    # Circle\n",
        "        4: 's',    # Square\n",
        "        6: '^',    # Triangle\n",
        "        8: 'D',    # Diamond\n",
        "    }\n",
        "    \n",
        "    # Filter for test accuracy data\n",
        "    test_df = accuracy_df[accuracy_df['accuracy_type'] == 'Test']\n",
        "    \n",
        "    # Create test accuracy plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    for cut_layer in sorted(test_df['cut_layer'].unique()):\n",
        "        for dp_type in sorted(test_df['dp_type'].unique()):\n",
        "            data = test_df[(test_df['cut_layer'] == cut_layer) & (test_df['dp_type'] == dp_type)]\n",
        "            if not data.empty:\n",
        "                label = f\"Cut Layer {cut_layer} - {dp_type}\"\n",
        "                plt.plot(data['round'], data['accuracy'], \n",
        "                        color=colors.get(cut_layer, '#000000'), \n",
        "                        linestyle=line_styles.get(dp_type, '-'),\n",
        "                        linewidth=2.5, \n",
        "                        marker=markers.get(cut_layer, 'o'), \n",
        "                        markersize=8,\n",
        "                        label=label)\n",
        "    \n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.title('Test Accuracy Comparison by Cut Layer', fontsize=18, fontweight='bold')\n",
        "    plt.xlabel('Training Round', fontsize=16)\n",
        "    plt.ylabel('Test Accuracy (%)', fontsize=16)\n",
        "    plt.legend(fontsize=12, frameon=True, facecolor='white', edgecolor='#CCCCCC', loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plt.savefig(\"cut_layer_test_accuracy.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Filter for validation accuracy data\n",
        "    val_df = accuracy_df[accuracy_df['accuracy_type'] == 'Validation']\n",
        "    \n",
        "    # Create validation accuracy plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    for cut_layer in sorted(val_df['cut_layer'].unique()):\n",
        "        for dp_type in sorted(val_df['dp_type'].unique()):\n",
        "            data = val_df[(val_df['cut_layer'] == cut_layer) & (val_df['dp_type'] == dp_type)]\n",
        "            if not data.empty:\n",
        "                label = f\"Cut Layer {cut_layer} - {dp_type}\"\n",
        "                plt.plot(data['round'], data['accuracy'], \n",
        "                        color=colors.get(cut_layer, '#000000'), \n",
        "                        linestyle=line_styles.get(dp_type, '-'),\n",
        "                        linewidth=2.5, \n",
        "                        marker=markers.get(cut_layer, 'o'), \n",
        "                        markersize=6, \n",
        "                        markevery=5,  # Show markers every 5 points to reduce clutter\n",
        "                        label=label)\n",
        "    \n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.title('Validation Accuracy Comparison by Cut Layer', fontsize=18, fontweight='bold')\n",
        "    plt.xlabel('Training Round', fontsize=16)\n",
        "    plt.ylabel('Validation Accuracy (%)', fontsize=16)\n",
        "    plt.legend(fontsize=12, frameon=True, facecolor='white', edgecolor='#CCCCCC', loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plt.savefig(\"cut_layer_validation_accuracy.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Try to load the final accuracy summary\n",
        "    try:\n",
        "        final_test_pivot = pd.read_csv('cut_layer_final_accuracy.csv')\n",
        "        print(\"Final Test Accuracies:\")\n",
        "        display(final_test_pivot)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Final accuracy summary file not found.\")\n",
        "else:\n",
        "    print(\"Please run the cut layer comparison experiments first to generate data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the baseline comparison results with Meta paper style (red coloring)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the Meta paper style\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "red_colors = ['#FF0000', '#CC0000', '#990000', '#660000', '#330000']  # Different shades of red\n",
        "\n",
        "# Load the results\n",
        "results_data = []\n",
        "histories = {}\n",
        "\n",
        "for exp in baseline_results:\n",
        "    try:\n",
        "        # Load metrics for this experiment\n",
        "        metrics_path = Path(project_root) / exp['metrics_path']\n",
        "        if not metrics_path.exists():\n",
        "            print(f\"Warning: No metrics found for {exp['run_id']}\")\n",
        "            continue\n",
        "            \n",
        "        with open(metrics_path, 'r') as f:\n",
        "            metrics = json.load(f)\n",
        "        \n",
        "        # Store the history for plotting\n",
        "        if 'history' in metrics:\n",
        "            histories[exp['description']] = {\n",
        "                'rounds': metrics['history']['round'],\n",
        "                'accuracy': metrics['history']['accuracy'],\n",
        "                'privacy_epsilon': metrics['history'].get('privacy_epsilon', [0] * len(metrics['history']['round']))\n",
        "            }\n",
        "        \n",
        "        # Extract key metrics\n",
        "        results_data.append({\n",
        "            'description': exp['description'],\n",
        "            'final_accuracy': metrics.get('final_test_acc', None),\n",
        "            'final_epsilon': metrics.get('privacy', {}).get('final_epsilon', None),\n",
        "            'runtime_minutes': exp['total_runtime'] / 60\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {exp['run_id']}: {e}\")\n",
        "\n",
        "# Create a figure with Meta paper style\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot accuracy over training rounds for each experiment\n",
        "for i, (desc, data) in enumerate(histories.items()):\n",
        "    plt.plot(data['rounds'], data['accuracy'], \n",
        "             color=red_colors[i % len(red_colors)], \n",
        "             linewidth=2.5, \n",
        "             marker='o', \n",
        "             markersize=6, \n",
        "             label=desc)\n",
        "\n",
        "# Set the style\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.title('Accuracy Comparison of Different DP Noise Settings', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Training Round', fontsize=14)\n",
        "plt.ylabel('Test Accuracy', fontsize=14)\n",
        "plt.legend(fontsize=12, frameon=True, facecolor='white', edgecolor='#CCCCCC')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Add a text box with final accuracies\n",
        "textbox = \"\"\n",
        "for result in sorted(results_data, key=lambda x: x['description']):\n",
        "    textbox += f\"{result['description']}: {result['final_accuracy']:.4f}\\n\"\n",
        "\n",
        "props = dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
        "plt.text(0.02, 0.02, textbox, transform=plt.gca().transAxes, fontsize=10,\n",
        "         verticalalignment='bottom', bbox=props)\n",
        "\n",
        "# Save the figure\n",
        "output_file = output_dir / \"baseline_accuracy_comparison.png\"\n",
        "plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Comparison plot saved to {output_file}\")\n",
        "\n",
        "# Create a second plot for privacy budget evolution\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot privacy budget over training rounds for each experiment\n",
        "for i, (desc, data) in enumerate(histories.items()):\n",
        "    if 'privacy_epsilon' in data and any(data['privacy_epsilon']):  # Only if privacy data exists\n",
        "        plt.plot(data['rounds'], data['privacy_epsilon'], \n",
        "                color=red_colors[i % len(red_colors)], \n",
        "                linewidth=2.5, \n",
        "                marker='o', \n",
        "                markersize=6, \n",
        "                label=desc)\n",
        "\n",
        "# Set the style\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.title('Privacy Budget Evolution of Different DP Noise Settings', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Training Round', fontsize=14)\n",
        "plt.ylabel('Privacy Budget (ε)', fontsize=14)\n",
        "plt.legend(fontsize=12, frameon=True, facecolor='white', edgecolor='#CCCCCC')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure\n",
        "privacy_output_file = output_dir / \"baseline_privacy_comparison.png\"\n",
        "plt.savefig(privacy_output_file, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Privacy comparison plot saved to {privacy_output_file}\")\n",
        "\n",
        "# Create a summary table\n",
        "import pandas as pd\n",
        "summary_df = pd.DataFrame(results_data)\n",
        "summary_df = summary_df.sort_values('description')\n",
        "summary_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "'seaborn-whitegrid' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m/opt/anaconda3/envs/zion/lib/python3.12/site-packages/matplotlib/style/core.py:129\u001b[0m, in \u001b[0;36muse\u001b[0;34m(style)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m/opt/anaconda3/envs/zion/lib/python3.12/site-packages/matplotlib/__init__.py:903\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[0;34m(fname, transform, fail_on_error)\u001b[0m\n\u001b[1;32m    902\u001b[0m rc_temp \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 903\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_or_url(fname) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/anaconda3/envs/zion/lib/python3.12/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/anaconda3/envs/zion/lib/python3.12/site-packages/matplotlib/__init__.py:880\u001b[0m, in \u001b[0;36m_open_file_or_url\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m    879\u001b[0m fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(fname)\n\u001b[0;32m--> 880\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fname, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seaborn-whitegrid'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define the Meta paper style with red coloring\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseaborn-whitegrid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m red_colors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#FF0000\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#E50000\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#CC0000\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#B20000\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#990000\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Different shades of red\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Function to extract accuracy and round data from experiment output\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/zion/lib/python3.12/site-packages/matplotlib/style/core.py:131\u001b[0m, in \u001b[0;36muse\u001b[0;34m(style)\u001b[0m\n\u001b[1;32m    129\u001b[0m         style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid package style, path of style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile, URL of style file, or library style name (library \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyles are listed in `style.available`)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    135\u001b[0m filtered \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m style:  \u001b[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001b[39;00m\n",
            "\u001b[0;31mOSError\u001b[0m: 'seaborn-whitegrid' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell is left empty intentionally to remove the experiment_configs list approach\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Execute Experiments\n",
        "\n",
        "Now we'll run all the experiments in sequence. For each experiment, we'll:\n",
        "1. Print experiment details\n",
        "2. Run the experiment using the train_secure_sfl.py script\n",
        "3. Save results to the output directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run an experiment with progress tracking\n",
        "def run_experiment(config_file, run_id, description):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Starting experiment: {description}\")\n",
        "    print(f\"Config file: {config_file}\")\n",
        "    print(f\"Run ID: {run_id}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    # Create timestamp for this run\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_id_with_timestamp = f\"{run_id}_{timestamp}\"\n",
        "    \n",
        "    # Run the experiment\n",
        "    start_time = time.time()\n",
        "    cmd = f\"python experiments/train_secure_sfl.py --config configs/{config_file} --run_id {run_id_with_timestamp}\"\n",
        "    print(f\"Command: {cmd}\")\n",
        "    !{cmd}\n",
        "    end_time = time.time()\n",
        "    \n",
        "    # Calculate total runtime\n",
        "    total_runtime = end_time - start_time\n",
        "    print(f\"\\nExperiment completed in {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)\")\n",
        "    \n",
        "    # Return experiment metadata\n",
        "    return {\n",
        "        \"config\": config_file,\n",
        "        \"run_id\": run_id_with_timestamp,\n",
        "        \"description\": description,\n",
        "        \"start_time\": start_time,\n",
        "        \"end_time\": end_time,\n",
        "        \"total_runtime\": total_runtime,\n",
        "        \"metrics_path\": f\"experiments/out/{run_id_with_timestamp}/metrics.json\"\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run and record all experiments from configs directory\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Get all config files directly\n",
        "config_files = sorted(glob.glob(\"configs/*.yaml\"))\n",
        "print(f\"Found {len(config_files)} configuration files to run\")\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# Create progress bar\n",
        "pbar = tqdm(total=len(config_files), desc=\"Experiments\")\n",
        "\n",
        "# Run each config file directly\n",
        "for i, config_path in enumerate(config_files):\n",
        "    # Extract filename without path and extension\n",
        "    config_file = os.path.basename(config_path)\n",
        "    run_id = config_file.replace('.yaml', '')\n",
        "    \n",
        "    # Generate a simple description from the filename\n",
        "    desc = f\"Experiment with {config_file}\"\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Running experiment {i+1}/{len(config_files)}\")\n",
        "    print(f\"Config file: {config_file}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Measure execution time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Run the experiment\n",
        "        cmd = f\"python experiments/train_secure_sfl.py --config {config_path} --run_id {run_id}\"\n",
        "        print(f\"Command: {cmd}\")\n",
        "        !{cmd}\n",
        "        \n",
        "        end_time = time.time()\n",
        "        total_runtime = end_time - start_time\n",
        "        \n",
        "        # Record result\n",
        "        result = {\n",
        "            \"config\": config_file,\n",
        "            \"run_id\": run_id,\n",
        "            \"description\": desc,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "            \"total_runtime\": total_runtime,\n",
        "            \"metrics_path\": f\"experiments/out/{run_id}/metrics.json\",\n",
        "            \"status\": \"completed\"\n",
        "        }\n",
        "        \n",
        "        print(f\"\\nExperiment completed in {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Handle errors\n",
        "        print(f\"\\n{'!'*80}\")\n",
        "        print(f\"Error running experiment with {config_file}: {str(e)}\")\n",
        "        print(f\"{'!'*80}\")\n",
        "        \n",
        "        end_time = time.time()\n",
        "        total_runtime = end_time - start_time\n",
        "        \n",
        "        # Record failure\n",
        "        result = {\n",
        "            \"config\": config_file,\n",
        "            \"run_id\": run_id,\n",
        "            \"description\": desc,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "            \"total_runtime\": total_runtime,\n",
        "            \"error\": str(e),\n",
        "            \"status\": \"failed\"\n",
        "        }\n",
        "    \n",
        "    # Add to results\n",
        "    all_results.append(result)\n",
        "    \n",
        "    # Update progress\n",
        "    pbar.update(1)\n",
        "\n",
        "# Close progress bar\n",
        "pbar.close()\n",
        "\n",
        "# Save experiment summary\n",
        "summary_file = output_dir / \"experiment_summary.json\"\n",
        "with open(summary_file, 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "    \n",
        "print(f\"\\nAll {len(config_files)} experiments completed\")\n",
        "print(f\"Summary saved to {summary_file}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Results Analysis and Visualization\n",
        "\n",
        "Now that all experiments are complete, let's analyze and visualize the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load experiment summary\n",
        "with open(summary_file, 'r') as f:\n",
        "    experiment_summary = json.load(f)\n",
        "    \n",
        "# Create a DataFrame for easier analysis\n",
        "results_data = []\n",
        "\n",
        "for exp in experiment_summary:\n",
        "    try:\n",
        "        # Load metrics for this experiment\n",
        "        metrics_path = Path(project_root) / exp['metrics_path']\n",
        "        if not metrics_path.exists():\n",
        "            print(f\"Warning: No metrics found for {exp['run_id']}\")\n",
        "            continue\n",
        "            \n",
        "        with open(metrics_path, 'r') as f:\n",
        "            metrics = json.load(f)\n",
        "        \n",
        "        # Extract key metrics\n",
        "        results_data.append({\n",
        "            'run_id': exp['run_id'],\n",
        "            'description': exp['description'],\n",
        "            'config': exp['config'],\n",
        "            'runtime_minutes': exp['total_runtime'] / 60,\n",
        "            'final_accuracy': metrics.get('final_test_acc', None),\n",
        "            'final_epsilon': metrics.get('privacy', {}).get('final_epsilon', None),\n",
        "            'dataset': metrics.get('config', {}).get('dataset', '').lower(),\n",
        "            'num_clients': metrics.get('config', {}).get('num_clients', 0),\n",
        "            'batch_size': metrics.get('config', {}).get('batch_size', 0),\n",
        "            'cut_layer': metrics.get('config', {}).get('cut_layer', 0),\n",
        "            'rounds': metrics.get('config', {}).get('num_rounds', 0),\n",
        "            'dp_mode': metrics.get('config', {}).get('dp_noise', {}).get('mode', 'unknown')\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {exp['run_id']}: {e}\")\n",
        "        \n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame(results_data)\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.1 Accuracy vs. Privacy Trade-off\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot accuracy vs privacy budget (epsilon)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=results_df, x='final_epsilon', y='final_accuracy', \n",
        "                hue='dataset', style='dp_mode', size='num_clients', \n",
        "                sizes=(50, 200), alpha=0.7)\n",
        "\n",
        "plt.title('Accuracy vs. Privacy Trade-off')\n",
        "plt.xlabel('Privacy Budget (ε)')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.savefig(output_dir / 'accuracy_vs_privacy.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.2 Client Scaling Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for client scaling experiments (MNIST with vanilla DP)\n",
        "client_scaling_df = results_df[results_df['config'].str.contains('mnist_clients') & \n",
        "                              results_df['config'].str.contains('vanilla')].sort_values('num_clients')\n",
        "\n",
        "# Create comparison plots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot accuracy vs number of clients\n",
        "sns.barplot(x='num_clients', y='final_accuracy', data=client_scaling_df, ax=ax1)\n",
        "ax1.set_title('Accuracy vs. Number of Clients')\n",
        "ax1.set_xlabel('Number of Clients')\n",
        "ax1.set_ylabel('Test Accuracy')\n",
        "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Plot runtime vs number of clients\n",
        "sns.barplot(x='num_clients', y='runtime_minutes', data=client_scaling_df, ax=ax2)\n",
        "ax2.set_title('Runtime vs. Number of Clients')\n",
        "ax2.set_xlabel('Number of Clients')\n",
        "ax2.set_ylabel('Runtime (minutes)')\n",
        "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'client_scaling_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.3 Split Strategy Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for split strategy experiments\n",
        "split_strategy_df = results_df[results_df['config'].str.contains('mnist') & \n",
        "                              (results_df['config'].str.contains('cut_layer') | \n",
        "                               results_df['run_id'].str.contains('clients5_vanilla'))]\n",
        "\n",
        "# Add a human-readable split strategy column\n",
        "def get_split_strategy(row):\n",
        "    if 'central' in row['run_id']:\n",
        "        return 'Centralized'\n",
        "    elif 'fl_sim' in row['run_id']:\n",
        "        return 'Federated Learning'\n",
        "    else:\n",
        "        return f\"Split Learning (cut={row['cut_layer']})\"\n",
        "\n",
        "split_strategy_df['split_strategy'] = split_strategy_df.apply(get_split_strategy, axis=1)\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create grouped bar plot\n",
        "metrics = ['final_accuracy', 'final_epsilon', 'runtime_minutes']\n",
        "metric_names = ['Test Accuracy', 'Privacy Budget (ε)', 'Runtime (minutes)']\n",
        "\n",
        "# Normalize values for better visualization\n",
        "normalized_df = split_strategy_df.copy()\n",
        "for metric in metrics:\n",
        "    max_val = normalized_df[metric].max()\n",
        "    if max_val > 0:  # Avoid division by zero\n",
        "        normalized_df[f'{metric}_norm'] = normalized_df[metric] / max_val\n",
        "\n",
        "# Create plot for each metric\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
        "    sns.barplot(x='split_strategy', y=metric, data=split_strategy_df, ax=axes[i])\n",
        "    axes[i].set_title(f'{name} by Split Strategy')\n",
        "    axes[i].set_xlabel('Split Strategy')\n",
        "    axes[i].set_ylabel(name)\n",
        "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'split_strategy_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.4 Adaptive vs. Vanilla DP Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for DP comparison experiments\n",
        "dp_comparison_df = results_df[\n",
        "    ((results_df['config'].str.contains('mnist_clients5_vanilla') | \n",
        "      results_df['config'].str.contains('mnist_clients5_adaptive')) |\n",
        "     (results_df['config'].str.contains('bcw_clients5_vanilla') | \n",
        "      results_df['config'].str.contains('bcw_clients5_adaptive')))\n",
        "]\n",
        "\n",
        "# Add a dataset-dp column for grouping\n",
        "dp_comparison_df['dataset_dp'] = dp_comparison_df.apply(\n",
        "    lambda row: f\"{row['dataset']}-{'adaptive' if 'adaptive' in row['run_id'] else 'vanilla'}\", \n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Plot accuracy evolution over time\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# For each experiment, load history and plot accuracy evolution\n",
        "for _, exp in dp_comparison_df.iterrows():\n",
        "    metrics_path = Path(project_root) / exp['metrics_path'].replace(exp['run_id'], exp['run_id'])\n",
        "    try:\n",
        "        with open(metrics_path, 'r') as f:\n",
        "            metrics = json.load(f)\n",
        "        \n",
        "        if 'history' in metrics:\n",
        "            rounds = metrics['history']['round']\n",
        "            accuracy = metrics['history']['accuracy']\n",
        "            \n",
        "            # Plot line\n",
        "            plt.plot(rounds, accuracy, marker='o', alpha=0.7, \n",
        "                     label=f\"{exp['dataset'].upper()} - {'Adaptive' if 'adaptive' in exp['run_id'] else 'Vanilla'} DP\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing history for {exp['run_id']}: {e}\")\n",
        "\n",
        "plt.title('Accuracy Evolution: Adaptive vs. Vanilla DP')\n",
        "plt.xlabel('Training Round')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.savefig(output_dir / 'dp_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.5 Privacy Budget Evolution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot privacy budget evolution for a few selected experiments\n",
        "selected_experiments = [\n",
        "    'mnist_clients5_vanilla',\n",
        "    'mnist_clients5_adaptive',\n",
        "    'mnist_fl_sim',\n",
        "    'bcw_clients5_vanilla'\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for exp_type in selected_experiments:\n",
        "    matching_exps = [exp for exp in experiment_summary if exp['run_id'].startswith(exp_type)]\n",
        "    \n",
        "    if matching_exps:\n",
        "        exp = matching_exps[0]\n",
        "        metrics_path = Path(project_root) / exp['metrics_path']\n",
        "        \n",
        "        try:\n",
        "            with open(metrics_path, 'r') as f:\n",
        "                metrics = json.load(f)\n",
        "            \n",
        "            if 'history' in metrics:\n",
        "                rounds = metrics['history']['round']\n",
        "                privacy_epsilon = metrics['history']['privacy_epsilon']\n",
        "                \n",
        "                plt.plot(rounds, privacy_epsilon, marker='o', alpha=0.7, label=exp_type)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing privacy history for {exp_type}: {e}\")\n",
        "\n",
        "plt.title('Privacy Budget Evolution')\n",
        "plt.xlabel('Training Round')\n",
        "plt.ylabel('Privacy Budget (ε)')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.savefig(output_dir / 'privacy_budget_evolution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Comprehensive Results Summary\n",
        "\n",
        "Finally, let's generate a comprehensive summary of all experiment results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a final summary table\n",
        "summary_table = results_df[['description', 'final_accuracy', 'final_epsilon', 'runtime_minutes', \n",
        "                           'num_clients', 'cut_layer', 'dp_mode']]\n",
        "\n",
        "# Rename columns for better readability\n",
        "summary_table = summary_table.rename(columns={\n",
        "    'description': 'Experiment',\n",
        "    'final_accuracy': 'Accuracy', \n",
        "    'final_epsilon': 'Privacy Budget (ε)', \n",
        "    'runtime_minutes': 'Runtime (min)',\n",
        "    'num_clients': 'Clients',\n",
        "    'cut_layer': 'Cut Layer',\n",
        "    'dp_mode': 'DP Mode'\n",
        "})\n",
        "\n",
        "# Format numbers\n",
        "summary_table['Accuracy'] = summary_table['Accuracy'].map(lambda x: f\"{x:.4f}\" if pd.notnull(x) else \"N/A\")\n",
        "summary_table['Privacy Budget (ε)'] = summary_table['Privacy Budget (ε)'].map(lambda x: f\"{x:.4f}\" if pd.notnull(x) else \"N/A\")\n",
        "summary_table['Runtime (min)'] = summary_table['Runtime (min)'].map(lambda x: f\"{x:.2f}\" if pd.notnull(x) else \"N/A\")\n",
        "\n",
        "# Display the table\n",
        "summary_table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save summary as HTML and CSV\n",
        "summary_table.to_html(output_dir / 'experiment_summary_table.html')\n",
        "summary_table.to_csv(output_dir / 'experiment_summary_table.csv')\n",
        "\n",
        "print(f\"Summary table saved to {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "This notebook ran a comprehensive suite of experiments for the SFL-DP framework, exploring various datasets, client configurations, split strategies, and DP mechanisms. Key findings from the experiments include:\n",
        "\n",
        "1. **Client Scaling**: As the number of clients increases, [describe observed trend]\n",
        "2. **Split Strategies**: The different split strategies (federated, centralized, SFL) showed [describe observations]\n",
        "3. **Adaptive vs. Vanilla DP**: The adaptive DP mechanism demonstrated [describe findings]\n",
        "4. **Dataset Influence**: MNIST and BCW datasets showed [describe differences]\n",
        "\n",
        "The visualizations provide a clear picture of the accuracy-privacy trade-offs as well as performance characteristics of different configurations.\n",
        "\n",
        "All results are saved to the output directory, and the plots can be found at the following location: `{output_dir}`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "zion",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
