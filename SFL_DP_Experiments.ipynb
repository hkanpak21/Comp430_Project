{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Secure Split Federated Learning (SFL) with DP - Experiment Suite\n",
        "\n",
        "This notebook provides an automated experiment environment for the SFL-DP framework. It will:\n",
        "1. Clone the repository\n",
        "2. Install dependencies\n",
        "3. Run experiments across various datasets, client configurations, SFL split strategies, and DP mechanisms\n",
        "4. Visualize and analyze the results\n",
        "\n",
        "**Note:** Make sure to run this notebook with GPU enabled (Runtime → Change runtime type → Hardware accelerator → GPU)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup Environment\n",
        "\n",
        "First, we'll check if we have GPU available, clone the repository, and install dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/YOUR_USERNAME/Comp430_Project.git\n",
        "%cd Comp430_Project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add repo path to Python path for imports\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# Set project root\n",
        "project_root = os.getcwd()\n",
        "sys.path.insert(0, project_root)\n",
        "print(f\"Project root: {project_root}\")\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path(project_root) / \"experiments\" / \"out\" / \"colab_runs\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Output directory: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Experiment Configuration\n",
        "\n",
        "Let's define our experiment suite. We'll run a combination of experiments with different:\n",
        "- Datasets (MNIST, BCW)\n",
        "- Client counts (3, 5, 10, 20)\n",
        "- DP mechanisms (vanilla, adaptive)\n",
        "- Split strategies (various cut layers, FL, centralized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List available configuration files\n",
        "config_files = list(Path(project_root).glob(\"configs/*.yaml\"))\n",
        "print(f\"Found {len(config_files)} configuration files:\")\n",
        "for config_file in sorted(config_files):\n",
        "    print(f\"- {config_file.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define our experiment suite\n",
        "experiment_configs = [\n",
        "    # MNIST with different client counts (vanilla DP)\n",
        "    {\"config\": \"mnist_clients3_vanilla_dp.yaml\", \"run_id\": \"mnist_clients3_vanilla\", \"desc\": \"MNIST with 3 clients (vanilla DP)\"},\n",
        "    {\"config\": \"mnist_clients5_vanilla_dp.yaml\", \"run_id\": \"mnist_clients5_vanilla\", \"desc\": \"MNIST with 5 clients (vanilla DP)\"},\n",
        "    {\"config\": \"mnist_clients10_vanilla_dp.yaml\", \"run_id\": \"mnist_clients10_vanilla\", \"desc\": \"MNIST with 10 clients (vanilla DP)\"},\n",
        "    {\"config\": \"mnist_clients20_vanilla_dp.yaml\", \"run_id\": \"mnist_clients20_vanilla\", \"desc\": \"MNIST with 20 clients (vanilla DP)\"},\n",
        "    \n",
        "    # MNIST with adaptive DP\n",
        "    {\"config\": \"mnist_clients5_adaptive_dp.yaml\", \"run_id\": \"mnist_clients5_adaptive\", \"desc\": \"MNIST with 5 clients (adaptive DP)\"},\n",
        "    \n",
        "    # Split strategies\n",
        "    {\"config\": \"mnist_cut_layer_fl.yaml\", \"run_id\": \"mnist_fl_sim\", \"desc\": \"MNIST simulating Federated Learning (cut at last layer)\"},\n",
        "    {\"config\": \"mnist_cut_layer_central.yaml\", \"run_id\": \"mnist_central_sim\", \"desc\": \"MNIST simulating Centralized Learning (cut at first layer)\"},\n",
        "    \n",
        "    # Breast Cancer Wisconsin dataset\n",
        "    {\"config\": \"bcw_clients5_vanilla_dp.yaml\", \"run_id\": \"bcw_clients5_vanilla\", \"desc\": \"BCW with 5 clients (vanilla DP)\"},\n",
        "    {\"config\": \"bcw_clients5_adaptive_dp.yaml\", \"run_id\": \"bcw_clients5_adaptive\", \"desc\": \"BCW with 5 clients (adaptive DP)\"}\n",
        "]\n",
        "\n",
        "print(f\"Defined {len(experiment_configs)} experiments to run\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Execute Experiments\n",
        "\n",
        "Now we'll run all the experiments in sequence. For each experiment, we'll:\n",
        "1. Print experiment details\n",
        "2. Run the experiment using the train_secure_sfl.py script\n",
        "3. Save results to the output directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run an experiment with progress tracking\n",
        "def run_experiment(config_file, run_id, description):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Starting experiment: {description}\")\n",
        "    print(f\"Config file: {config_file}\")\n",
        "    print(f\"Run ID: {run_id}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    # Create timestamp for this run\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_id_with_timestamp = f\"{run_id}_{timestamp}\"\n",
        "    \n",
        "    # Run the experiment\n",
        "    start_time = time.time()\n",
        "    cmd = f\"python experiments/train_secure_sfl.py --config configs/{config_file} --run_id {run_id_with_timestamp}\"\n",
        "    print(f\"Command: {cmd}\")\n",
        "    !{cmd}\n",
        "    end_time = time.time()\n",
        "    \n",
        "    # Calculate total runtime\n",
        "    total_runtime = end_time - start_time\n",
        "    print(f\"\\nExperiment completed in {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)\")\n",
        "    \n",
        "    # Return experiment metadata\n",
        "    return {\n",
        "        \"config\": config_file,\n",
        "        \"run_id\": run_id_with_timestamp,\n",
        "        \"description\": description,\n",
        "        \"start_time\": start_time,\n",
        "        \"end_time\": end_time,\n",
        "        \"total_runtime\": total_runtime,\n",
        "        \"metrics_path\": f\"experiments/out/{run_id_with_timestamp}/metrics.json\"\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all experiments\n",
        "all_results = []\n",
        "\n",
        "for i, experiment in enumerate(experiment_configs):\n",
        "    print(f\"\\nRunning experiment {i+1}/{len(experiment_configs)}\")\n",
        "    result = run_experiment(\n",
        "        experiment['config'],\n",
        "        experiment['run_id'],\n",
        "        experiment['desc']\n",
        "    )\n",
        "    all_results.append(result)\n",
        "    \n",
        "# Save experiment summary\n",
        "summary_file = output_dir / \"experiment_summary.json\"\n",
        "with open(summary_file, 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "    \n",
        "print(f\"\\nAll {len(experiment_configs)} experiments completed\")\n",
        "print(f\"Summary saved to {summary_file}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Results Analysis and Visualization\n",
        "\n",
        "Now that all experiments are complete, let's analyze and visualize the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load experiment summary\n",
        "with open(summary_file, 'r') as f:\n",
        "    experiment_summary = json.load(f)\n",
        "    \n",
        "# Create a DataFrame for easier analysis\n",
        "results_data = []\n",
        "\n",
        "for exp in experiment_summary:\n",
        "    try:\n",
        "        # Load metrics for this experiment\n",
        "        metrics_path = Path(project_root) / exp['metrics_path']\n",
        "        if not metrics_path.exists():\n",
        "            print(f\"Warning: No metrics found for {exp['run_id']}\")\n",
        "            continue\n",
        "            \n",
        "        with open(metrics_path, 'r') as f:\n",
        "            metrics = json.load(f)\n",
        "        \n",
        "        # Extract key metrics\n",
        "        results_data.append({\n",
        "            'run_id': exp['run_id'],\n",
        "            'description': exp['description'],\n",
        "            'config': exp['config'],\n",
        "            'runtime_minutes': exp['total_runtime'] / 60,\n",
        "            'final_accuracy': metrics.get('final_test_acc', None),\n",
        "            'final_epsilon': metrics.get('privacy', {}).get('final_epsilon', None),\n",
        "            'dataset': metrics.get('config', {}).get('dataset', '').lower(),\n",
        "            'num_clients': metrics.get('config', {}).get('num_clients', 0),\n",
        "            'batch_size': metrics.get('config', {}).get('batch_size', 0),\n",
        "            'cut_layer': metrics.get('config', {}).get('cut_layer', 0),\n",
        "            'rounds': metrics.get('config', {}).get('num_rounds', 0),\n",
        "            'dp_mode': metrics.get('config', {}).get('dp_noise', {}).get('mode', 'unknown')\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {exp['run_id']}: {e}\")\n",
        "        \n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame(results_data)\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.1 Accuracy vs. Privacy Trade-off\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot accuracy vs privacy budget (epsilon)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=results_df, x='final_epsilon', y='final_accuracy', \n",
        "                hue='dataset', style='dp_mode', size='num_clients', \n",
        "                sizes=(50, 200), alpha=0.7)\n",
        "\n",
        "plt.title('Accuracy vs. Privacy Trade-off')\n",
        "plt.xlabel('Privacy Budget (ε)')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.savefig(output_dir / 'accuracy_vs_privacy.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.2 Client Scaling Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for client scaling experiments (MNIST with vanilla DP)\n",
        "client_scaling_df = results_df[results_df['config'].str.contains('mnist_clients') & \n",
        "                              results_df['config'].str.contains('vanilla')].sort_values('num_clients')\n",
        "\n",
        "# Create comparison plots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot accuracy vs number of clients\n",
        "sns.barplot(x='num_clients', y='final_accuracy', data=client_scaling_df, ax=ax1)\n",
        "ax1.set_title('Accuracy vs. Number of Clients')\n",
        "ax1.set_xlabel('Number of Clients')\n",
        "ax1.set_ylabel('Test Accuracy')\n",
        "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Plot runtime vs number of clients\n",
        "sns.barplot(x='num_clients', y='runtime_minutes', data=client_scaling_df, ax=ax2)\n",
        "ax2.set_title('Runtime vs. Number of Clients')\n",
        "ax2.set_xlabel('Number of Clients')\n",
        "ax2.set_ylabel('Runtime (minutes)')\n",
        "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'client_scaling_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.3 Split Strategy Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for split strategy experiments\n",
        "split_strategy_df = results_df[results_df['config'].str.contains('mnist') & \n",
        "                              (results_df['config'].str.contains('cut_layer') | \n",
        "                               results_df['run_id'].str.contains('clients5_vanilla'))]\n",
        "\n",
        "# Add a human-readable split strategy column\n",
        "def get_split_strategy(row):\n",
        "    if 'central' in row['run_id']:\n",
        "        return 'Centralized'\n",
        "    elif 'fl_sim' in row['run_id']:\n",
        "        return 'Federated Learning'\n",
        "    else:\n",
        "        return f\"Split Learning (cut={row['cut_layer']})\"\n",
        "\n",
        "split_strategy_df['split_strategy'] = split_strategy_df.apply(get_split_strategy, axis=1)\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create grouped bar plot\n",
        "metrics = ['final_accuracy', 'final_epsilon', 'runtime_minutes']\n",
        "metric_names = ['Test Accuracy', 'Privacy Budget (ε)', 'Runtime (minutes)']\n",
        "\n",
        "# Normalize values for better visualization\n",
        "normalized_df = split_strategy_df.copy()\n",
        "for metric in metrics:\n",
        "    max_val = normalized_df[metric].max()\n",
        "    if max_val > 0:  # Avoid division by zero\n",
        "        normalized_df[f'{metric}_norm'] = normalized_df[metric] / max_val\n",
        "\n",
        "# Create plot for each metric\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
        "    sns.barplot(x='split_strategy', y=metric, data=split_strategy_df, ax=axes[i])\n",
        "    axes[i].set_title(f'{name} by Split Strategy')\n",
        "    axes[i].set_xlabel('Split Strategy')\n",
        "    axes[i].set_ylabel(name)\n",
        "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'split_strategy_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.4 Adaptive vs. Vanilla DP Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for DP comparison experiments\n",
        "dp_comparison_df = results_df[\n",
        "    ((results_df['config'].str.contains('mnist_clients5_vanilla') | \n",
        "      results_df['config'].str.contains('mnist_clients5_adaptive')) |\n",
        "     (results_df['config'].str.contains('bcw_clients5_vanilla') | \n",
        "      results_df['config'].str.contains('bcw_clients5_adaptive')))\n",
        "]\n",
        "\n",
        "# Add a dataset-dp column for grouping\n",
        "dp_comparison_df['dataset_dp'] = dp_comparison_df.apply(\n",
        "    lambda row: f\"{row['dataset']}-{'adaptive' if 'adaptive' in row['run_id'] else 'vanilla'}\", \n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Plot accuracy evolution over time\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# For each experiment, load history and plot accuracy evolution\n",
        "for _, exp in dp_comparison_df.iterrows():\n",
        "    metrics_path = Path(project_root) / exp['metrics_path'].replace(exp['run_id'], exp['run_id'])\n",
        "    try:\n",
        "        with open(metrics_path, 'r') as f:\n",
        "            metrics = json.load(f)\n",
        "        \n",
        "        if 'history' in metrics:\n",
        "            rounds = metrics['history']['round']\n",
        "            accuracy = metrics['history']['accuracy']\n",
        "            \n",
        "            # Plot line\n",
        "            plt.plot(rounds, accuracy, marker='o', alpha=0.7, \n",
        "                     label=f\"{exp['dataset'].upper()} - {'Adaptive' if 'adaptive' in exp['run_id'] else 'Vanilla'} DP\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing history for {exp['run_id']}: {e}\")\n",
        "\n",
        "plt.title('Accuracy Evolution: Adaptive vs. Vanilla DP')\n",
        "plt.xlabel('Training Round')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.savefig(output_dir / 'dp_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.5 Privacy Budget Evolution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot privacy budget evolution for a few selected experiments\n",
        "selected_experiments = [\n",
        "    'mnist_clients5_vanilla',\n",
        "    'mnist_clients5_adaptive',\n",
        "    'mnist_fl_sim',\n",
        "    'bcw_clients5_vanilla'\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for exp_type in selected_experiments:\n",
        "    matching_exps = [exp for exp in experiment_summary if exp['run_id'].startswith(exp_type)]\n",
        "    \n",
        "    if matching_exps:\n",
        "        exp = matching_exps[0]\n",
        "        metrics_path = Path(project_root) / exp['metrics_path']\n",
        "        \n",
        "        try:\n",
        "            with open(metrics_path, 'r') as f:\n",
        "                metrics = json.load(f)\n",
        "            \n",
        "            if 'history' in metrics:\n",
        "                rounds = metrics['history']['round']\n",
        "                privacy_epsilon = metrics['history']['privacy_epsilon']\n",
        "                \n",
        "                plt.plot(rounds, privacy_epsilon, marker='o', alpha=0.7, label=exp_type)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing privacy history for {exp_type}: {e}\")\n",
        "\n",
        "plt.title('Privacy Budget Evolution')\n",
        "plt.xlabel('Training Round')\n",
        "plt.ylabel('Privacy Budget (ε)')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.savefig(output_dir / 'privacy_budget_evolution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Comprehensive Results Summary\n",
        "\n",
        "Finally, let's generate a comprehensive summary of all experiment results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a final summary table\n",
        "summary_table = results_df[['description', 'final_accuracy', 'final_epsilon', 'runtime_minutes', \n",
        "                           'num_clients', 'cut_layer', 'dp_mode']]\n",
        "\n",
        "# Rename columns for better readability\n",
        "summary_table = summary_table.rename(columns={\n",
        "    'description': 'Experiment',\n",
        "    'final_accuracy': 'Accuracy', \n",
        "    'final_epsilon': 'Privacy Budget (ε)', \n",
        "    'runtime_minutes': 'Runtime (min)',\n",
        "    'num_clients': 'Clients',\n",
        "    'cut_layer': 'Cut Layer',\n",
        "    'dp_mode': 'DP Mode'\n",
        "})\n",
        "\n",
        "# Format numbers\n",
        "summary_table['Accuracy'] = summary_table['Accuracy'].map(lambda x: f\"{x:.4f}\" if pd.notnull(x) else \"N/A\")\n",
        "summary_table['Privacy Budget (ε)'] = summary_table['Privacy Budget (ε)'].map(lambda x: f\"{x:.4f}\" if pd.notnull(x) else \"N/A\")\n",
        "summary_table['Runtime (min)'] = summary_table['Runtime (min)'].map(lambda x: f\"{x:.2f}\" if pd.notnull(x) else \"N/A\")\n",
        "\n",
        "# Display the table\n",
        "summary_table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save summary as HTML and CSV\n",
        "summary_table.to_html(output_dir / 'experiment_summary_table.html')\n",
        "summary_table.to_csv(output_dir / 'experiment_summary_table.csv')\n",
        "\n",
        "print(f\"Summary table saved to {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "This notebook ran a comprehensive suite of experiments for the SFL-DP framework, exploring various datasets, client configurations, split strategies, and DP mechanisms. Key findings from the experiments include:\n",
        "\n",
        "1. **Client Scaling**: As the number of clients increases, [describe observed trend]\n",
        "2. **Split Strategies**: The different split strategies (federated, centralized, SFL) showed [describe observations]\n",
        "3. **Adaptive vs. Vanilla DP**: The adaptive DP mechanism demonstrated [describe findings]\n",
        "4. **Dataset Influence**: MNIST and BCW datasets showed [describe differences]\n",
        "\n",
        "The visualizations provide a clear picture of the accuracy-privacy trade-offs as well as performance characteristics of different configurations.\n",
        "\n",
        "All results are saved to the output directory, and the plots can be found at the following location: `{output_dir}`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
